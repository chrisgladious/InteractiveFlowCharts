{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f40d55",
   "metadata": {},
   "source": [
    "# Activate Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb41e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .\\.venv\\Scripts\\Activate.ps1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad5a15",
   "metadata": {},
   "source": [
    "# Methods to Import and Cache csv or xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "def dprint(x): # https://stackoverflow.com/questions/32000934/print-a-variables-name-and-value/57225950#57225950\n",
    "    frame = inspect.currentframe().f_back\n",
    "    s = inspect.getframeinfo(frame).code_context[0]\n",
    "    r = re.search(r\"\\((.*)\\)\", s).group(1)\n",
    "    print(\"{} = {}\".format(r,x))\n",
    "\n",
    "def _pk1_path_for_file(file_path):\n",
    "    \"\"\"Return Path for .pk1 cache file stored next to the input file with same base name and extension '.pk1'.\"\"\"\n",
    "    p = Path(file_path) if not isinstance(file_path, Path) else file_path\n",
    "    return p.with_suffix('.pk1')\n",
    "\n",
    "def load_or_cache_excel(xlsx_path, read_kwargs=None, force_refresh=False):\n",
    "    \"\"\"Load DataFrame from a .pk1 cache next to the xlsx if present; otherwise read the xlsx and save the .pk1.\n",
    "    Returns the DataFrame.\n",
    "    read_kwargs: dict forwarded to pd.read_excel.\n",
    "    force_refresh: if True, re-read the Excel and overwrite cache.\"\"\"\n",
    "    read_kwargs = read_kwargs or {}\n",
    "    pk1 = _pk1_path_for_file(xlsx_path)\n",
    "    if pk1.exists() and not force_refresh:\n",
    "        try:\n",
    "            df = pd.read_pickle(pk1)\n",
    "            print(f'Loaded cache {pk1}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to load {pk1} (will re-read Excel): {e}')\n",
    "    # read Excel and attempt to save cache\n",
    "    df = pd.read_excel(xlsx_path, **read_kwargs)\n",
    "    try:\n",
    "        df.to_pickle(pk1)\n",
    "        print(f'Saved cache {pk1}')\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not save cache {pk1}: {e}')\n",
    "    return df\n",
    "\n",
    "def load_or_cache_csv(csv_path, read_kwargs=None, force_refresh=False):\n",
    "    \"\"\"Load DataFrame from a .pk1 cache next to the csv if present; otherwise read the csv and save the .pk1.\n",
    "    Returns the DataFrame.\n",
    "    read_kwargs: dict forwarded to pd.read_csv.\n",
    "    force_refresh: if True, re-read the CSV and overwrite cache.\"\"\"\n",
    "    read_kwargs = read_kwargs or {}\n",
    "    # Set default read parameters for our specific CSV format\n",
    "    default_params = {\n",
    "        'sep': ';',  # semicolon separated\n",
    "        'decimal': ',',  # comma as decimal separator\n",
    "        'parse_dates': ['TimeDate'],  # parse TimeDate column as datetime\n",
    "    }\n",
    "    # Update with any user-provided parameters\n",
    "    read_kwargs = {**default_params, **read_kwargs}\n",
    "    \n",
    "    pk1 = _pk1_path_for_file(csv_path)\n",
    "    if pk1.exists() and not force_refresh:\n",
    "        try:\n",
    "            df = pd.read_pickle(pk1)\n",
    "            print(f'Loaded cache {pk1}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to load {pk1} (will re-read CSV): {e}')\n",
    "    # read CSV and attempt to save cache\n",
    "    df = pd.read_csv(csv_path, **read_kwargs)\n",
    "    try:\n",
    "        df.to_pickle(pk1)\n",
    "        print(f'Saved cache {pk1}')\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not save cache {pk1}: {e}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c934f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files (with pk1 cache next to each csv). Uses load_or_cache_csv from previous cell.\n",
    "csv_file_path_AllaFlöden = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_Alla_Floden.csv'\n",
    "csv_file_path_Dosering = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_Dosering.csv'\n",
    "csv_file_path_Försedimentering = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_Försedimentering.csv'\n",
    "csv_file_path_MBBR = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_MBBR.csv'\n",
    "csv_file_path_Skivfilter = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_Skivfilter.csv'\n",
    "csv_file_path_Slambehandling = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\04 Underkonsult, Sidokonsult\\gefasystem.app.box.com\\20251113\\20251113_Slambehandling.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a710298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using the helper which places the .pk1 next to the csv with same base name\n",
    "df_AllaFlöden= load_or_cache_csv(csv_file_path_AllaFlöden)\n",
    "df_Dosering = load_or_cache_csv(csv_file_path_Dosering)\n",
    "df_Försedimentering= load_or_cache_csv(csv_file_path_Försedimentering)\n",
    "df_MBBR = load_or_cache_csv(csv_file_path_MBBR)\n",
    "df_Skivfilter = load_or_cache_csv(csv_file_path_Skivfilter)\n",
    "df_Slambehandling = load_or_cache_csv(csv_file_path_Slambehandling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f527bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple dataframes\n",
    "for df in [df_AllaFlöden, df_Dosering, df_Försedimentering, df_MBBR, df_Skivfilter, df_Slambehandling]:\n",
    "    df.rename(columns={'TimeDate' : 'DateTime'}, inplace=True)\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "    df.drop(columns=['ID', 'TimeLength'], inplace=True, errors='ignore')\n",
    "    # df.rename(columns=lambda x: x.replace('PajalaARV-AS01.AnalogIn.', ''), inplace=True)\n",
    "    df.rename(columns=lambda x: x.split('.')[-1], inplace=True)\n",
    "    # df.rename(columns=lambda x: x.replace('.*\\.', '', regex=True), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1937f6b",
   "metadata": {},
   "source": [
    "# Renaming imported Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ede93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataframe columns\n",
    "#TimeDate;TimeLength;FT10101;FT30101;FT72101;FT80101;LT23101\n",
    "df_AllaFlöden.rename(columns={\n",
    "    'FT10101': 'Inflöde FT10101 [m3/h]',\n",
    "    'FT30101': 'MBBRflöde FT30101 [m3/h]',\n",
    "    'FT72101': 'Utflöde FT72101 [m3/h]',\n",
    "    'FT80101': 'Inflöde Extenslam FT80101 uncalib [m3/h]',\n",
    "    'LT23101': 'Nivå Bräddning LT23101 [m]'}, inplace=True)\n",
    "df_AllaFlöden.drop(columns=['Nivå Bräddning LT23101 [m]'], inplace=True, errors='ignore')\n",
    "\n",
    "#TimeDate;TimeLength;FT90101;FT92101;FT92201;LT90101;LT91101;LT91201;LT91301\n",
    "df_Dosering.rename(columns={\n",
    "    'FT90101':'FällPAX FT90101 [l/h]',\n",
    "    'FT92101':'FlockPoly FT92101 [l/h]',\n",
    "    'FT92201':'SlamPuckPoly FT92201 [l/h]',\n",
    "    'LT90101' : 'NivåPAX LT90101 [m]',\n",
    "    'LT91101':'FlockPoly LT91101 [m]',\n",
    "    'LT91201':'SlamPuckPoly LT91201 [m]',\n",
    "    'LT91301':'SlamAvvatPoly LT91301 [m]',\n",
    "    # 'FT92301':'AvvatPoly FT92301 [l/h]'\n",
    "    }, inplace=True)\n",
    "\n",
    "# TimeDate;TimeLength;FT10101;LT20103;LT21101;LT23101;P22101;P22201\n",
    "df_Försedimentering.rename(columns={\n",
    "    'FT10101':'Inflöde FT10101 [m3/h]',\n",
    "    'LT20103':'FörsedUtlopp LT20103 [m]',\n",
    "    'LT21101':'FlytSlam LT21101 [m]',\n",
    "    'LT23101':'Nivå Bräddning LT23101 [m]',\n",
    "    'P22101':'Returslam P22101 [1/0]',\n",
    "    'P22201':'Returslam P22201 [1/0]'}, inplace=True)\n",
    "df_Försedimentering.drop(columns=['Inflöde FT10101 [m3/h]'], inplace=True, errors='ignore')\n",
    "\n",
    "# TimeDate;TimeLength;QT31101;LT31101;QT31102;QT31201;LT31201;QT31202\n",
    "df_MBBR.rename(columns={\n",
    "    'QT31101':'O2MBBR QT31101 [ppm]',\n",
    "    'LT31101':'MBBR LT31101 [m]',\n",
    "    'QT31102':'O2MBBR QT31102 [ppm]',\n",
    "    'QT31201':'O2MBBR QT31201 [ppm]',\n",
    "    'LT31201':'MBBR LT31201 [m]',\n",
    "    'QT31202':'O2MBBR QT31202 [ppm]'}, inplace=True)\n",
    "\n",
    "# TimeDate;TimeLength;LT70101;P70101_FQ1;PS70101;LT70102;LT70201;P70201_FQ1;PS70201;LT70202;LT71101\n",
    "df_Skivfilter.rename(columns={\n",
    "    'LT70101':'SFIn LT70101 [m]',\n",
    "    'P70101_FQ1':'SFS P70101_FQ1 [Hz]',\n",
    "    'PS70101':'SFSpol PS70101 [1/0]',\n",
    "    'LT70102':'SFUt LT70102 [m]',\n",
    "    'LT70201':'SFIn LT70201 [m]',\n",
    "    'P70201_FQ1':'SFS P70201_FQ1 [Hz]',\n",
    "    'PS70201':'SFS PS70201 [1/0]',\n",
    "    'LT70202':'SFUt LT70202 [m]',\n",
    "    'LT71101':'SFÖverSlam LT71101 [m]'}, inplace=True)\n",
    "\n",
    "# TimeDate;TimeLength;FT80101;FT81101;CV80101;LT80102;M80103_FQ;P82104_FQ1;P81101_FQ;P81102_FQ;P81101;P81102;FT82101;P82101_FQ1;P82102_FQ1;P82101;P82102;LT81101;LT82101;LT84101;P84101_FQ1;P84102_FQ1;P84101;P84102\n",
    "df_Slambehandling.rename(columns={\n",
    "    'FT80101':'Inflöde Extenslam FT80101 uncalib [m3/h]',\n",
    "    'FT81101':'PuckS FT81101 [m3/h]',\n",
    "    'CV80101':'ExternS CV80101 [%]',\n",
    "    'LT80102':'ExternS tråg LT80102 [m]',\n",
    "    'M80103_FQ':'ExternRens M80103_FQ [Hz]',\n",
    "    'P82104_FQ1':'AvvattnPolyMix P82104_FQ1 [Hz]',\n",
    "    'P81101_FQ':'Puck P81101_FQ [Hz]',\n",
    "    'P81102_FQ':'Puck P81102_FQ [Hz]',\n",
    "    'P81101':'Puck P81101 [1/0]',\n",
    "    'P81102':'Puck P81102 [1/0]',\n",
    "    'FT82101':'SlamAvvattn FT82101 [m3/h]',\n",
    "    'P82101_FQ1':'SlamAvvattn P82101_FQ1 [Hz]',\n",
    "    'P82102_FQ1':'SlamAvvattn P82102_FQ1 [Hz]',\n",
    "    'P82101': 'SlamAvvattn P82101 [1/0]',\n",
    "    'P82102': 'SlamAvvattn P82102 [1/0]',\n",
    "    'LT81101': 'Slamlager 1 LT81101 [m]',\n",
    "    'LT82101':  'Slamlager 2 LT82101 [m]',\n",
    "    'LT84101': 'Rejekt LT84101 [m]',\n",
    "    'P84101_FQ1': 'Rejekt P84101_FQ1 [Hz]',\n",
    "    'P84102_FQ1': 'Rejekt P84102_FQ1 [Hz]',\n",
    "    'P84101': 'Rejekt P84101 [1/0]',\n",
    "    'P84102': 'Rejekt P84102 [1/0]',\n",
    "    }, inplace=True)\n",
    "\n",
    "df_PlusNivåer = pd.DataFrame()\n",
    "df_PlusNivåer['FörsedUtlopp LT20103 [+m]'] = df_Försedimentering['FörsedUtlopp LT20103 [m]'] + 164 #164.069\n",
    "df_PlusNivåer['Nivå Bräddning LT23101 [+m]'] = df_Försedimentering['Nivå Bräddning LT23101 [m]'] + 163.100 #Golvnivå NWD\n",
    "df_PlusNivåer['FlytSlam LT21101 [+m]'] = df_Försedimentering['FlytSlam LT21101 [m]'] + 160.560 #Golvnivå NWD\n",
    "df_PlusNivåer['MBBR LT31101 [+m]'] = df_MBBR['MBBR LT31101 [m]'] + 158.510 #Golvnivå NWD\n",
    "df_PlusNivåer['MBBR LT31201 [+m]'] = df_MBBR['MBBR LT31201 [m]'] + 158.510 #Golvnivå NWD\n",
    "df_PlusNivåer['SFIn LT70101 [+m]'] = df_Skivfilter['SFIn LT70101 [m]'] +162.020 #Golvnivå NWD\n",
    "df_PlusNivåer['SFUt LT70102 [+m]'] = df_Skivfilter['SFUt LT70102 [m]'] +162.020 #Golvnivå NWD\n",
    "df_PlusNivåer['SFIn LT70201 [+m]'] = df_Skivfilter['SFIn LT70201 [m]'] +162.020 #Golvnivå NWD\n",
    "df_PlusNivåer['SFUt LT70202 [+m]'] = df_Skivfilter['SFUt LT70202 [m]'] +162.020 #Golvnivå NWD\n",
    "df_PlusNivåer['SFÖverSlam LT71101 [+m]'] = df_Skivfilter['SFÖverSlam LT71101 [m]'] +158.510 #Golvnivå NWD\n",
    "df_PlusNivåer['ExternS tråg LT80102 [+m]'] = df_Slambehandling['ExternS tråg LT80102 [m]']/1000 +165.241 #Gissning centrum rejektrör\n",
    "df_PlusNivåer['Slamlager 1 LT81101 [+m]'] = df_Slambehandling['Slamlager 1 LT81101 [m]'] + 160.050 #Golvnivå NWD\n",
    "df_PlusNivåer['Slamlager 2 LT82101 [+m]'] = df_Slambehandling['Slamlager 2 LT82101 [m]'] + 160.050 #Golvnivå NWD\n",
    "df_PlusNivåer['Rejekt LT84101 [+m]'] = df_Slambehandling['Rejekt LT84101 [m]'] + 163.954 #Golvnivå NWD\n",
    "\n",
    "df_Slambehandling.drop(columns=['Inflöde Extenslam FT80101 uncalib [m3/h]'], inplace=True)\n",
    "df_AllaFlöden['Inflöde Extenslam FT80101 [m3/h]'] = df_AllaFlöden['Inflöde Extenslam FT80101 uncalib [m3/h]'] + 0.486474692821503\n",
    "\n",
    "df_Inflöde_FT10101 = df_AllaFlöden[['Inflöde FT10101 [m3/h]']]\n",
    "df_Utflöde_FT72101 = df_AllaFlöden[['Utflöde FT72101 [m3/h]']]\n",
    "df_Inflöde_Extenslam_FT80101 = df_AllaFlöden[['Inflöde Extenslam FT80101 [m3/h]']]\n",
    "df_Nivå_Bräddning_LT23101 = df_Försedimentering[['Nivå Bräddning LT23101 [m]']]\n",
    "df_MBBRflöde_FT30101 = df_AllaFlöden[['MBBRflöde FT30101 [m3/h]']]\n",
    "\n",
    "# Merge the DataFrames on the DateTime index, aligning values\n",
    "# Using merge instead of concat to handle any duplicate indices\n",
    "# Merge all DataFrames sequentially\n",
    "# Tidigare df_ax\n",
    "# df_ax = pd.merge(df_Inflöde_FT10101, df_Utflöde_FT72101, left_index=True, right_index=True, how='outer')\n",
    "# df_ax = pd.merge(df_ax, df_Inflöde_Extenslam_FT80101, left_index=True, right_index=True, how='outer')\n",
    "# df_ax = pd.merge(df_ax, df_MBBRflöde_FT30101, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Nya df_ax\n",
    "df_ax = df_AllaFlöden\n",
    "df_ax = pd.merge(df_ax,df_Dosering, left_index=True, right_index=True, how='outer')\n",
    "df_ax = pd.merge(df_ax,df_Försedimentering, left_index=True, right_index=True, how='outer')\n",
    "df_ax = pd.merge(df_ax,df_MBBR, left_index=True, right_index=True, how='outer')\n",
    "df_ax = pd.merge(df_ax,df_Skivfilter, left_index=True, right_index=True, how='outer')\n",
    "df_ax = pd.merge(df_ax,df_Slambehandling, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "df_ax = df_ax.dropna(axis=1, how='all')\n",
    "\n",
    "# Show the result\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "dprint(df_ax.head())\n",
    "\n",
    "# Check for any missing values after merge\n",
    "print(\"\\nMissing values in merged DataFrame:\")\n",
    "dprint(df_ax.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read CSV files (with pk1 cache next to each csv). Uses load_or_cache_csv from previous cell.\n",
    "# csv_file_path_FT10101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT10101.csv'\n",
    "# csv_file_path_FT30101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT30101.csv'\n",
    "# csv_file_path_FT72101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT72101.csv'\n",
    "# csv_file_path_FT80101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT80101.csv'\n",
    "# csv_file_path_LT23101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\LT23101.csv'\n",
    "\n",
    "# # Load using the helper which places the .pk1 next to the csv with same base name\n",
    "# df_Inflöde_FT10101 = load_or_cache_csv(csv_file_path_FT10101) #Inflöde\n",
    "# df_Utflöde_FT72101 = load_or_cache_csv(csv_file_path_FT72101) #Utflöde\n",
    "# df_MBBRflöde_FT30101 = load_or_cache_csv(csv_file_path_FT30101) #MBBR-flöde\n",
    "# df_Inflöde_Extenslam_FT80101 = load_or_cache_csv(csv_file_path_FT80101) #Inflöde Extenslam\n",
    "# df_Nivå_Bräddning_LT23101 = load_or_cache_csv(csv_file_path_LT23101) #Utflöde Bräddning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dprint(df_1.head())\n",
    "# df_Inflöde_FT10101.rename(columns={'Val':'Inflöde FT10101'}, inplace=True)\n",
    "# df_Inflöde_FT10101['DateTime'] = pd.to_datetime(df_Inflöde_FT10101['DateTime'])\n",
    "# df_Inflöde_FT10101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "# df_Inflöde_FT10101.set_index('DateTime', inplace=True)\n",
    "# # dprint(df_1.head())\n",
    "\n",
    "\n",
    "# # print(df_2.head())\n",
    "# df_Utflöde_FT72101.rename(columns={'Val':'Utflöde FT72101'}, inplace=True)\n",
    "# df_Utflöde_FT72101['DateTime'] = pd.to_datetime(df_Utflöde_FT72101['DateTime'])\n",
    "# df_Utflöde_FT72101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "# df_Utflöde_FT72101.set_index('DateTime', inplace=True)\n",
    "# dprint(df_Utflöde_FT72101.head())\n",
    "\n",
    "# df_Inflöde_Extenslam_FT80101.rename(columns={'TimeDate': 'DateTime', 'Val':'Inflöde Extenslam FT80101'}, inplace=True)\n",
    "# df_Inflöde_Extenslam_FT80101['DateTime'] = pd.to_datetime(df_Inflöde_Extenslam_FT80101['DateTime'])\n",
    "# df_Inflöde_Extenslam_FT80101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "# df_Inflöde_Extenslam_FT80101.set_index('DateTime', inplace=True)\n",
    "# dprint(df_Inflöde_Extenslam_FT80101.head())\n",
    "# df_Inflöde_Extenslam_FT80101_before_zeroflow_calib = df_Inflöde_Extenslam_FT80101\n",
    "# df_Inflöde_Extenslam_FT80101['Inflöde Extenslam FT80101'] = df_Inflöde_Extenslam_FT80101['Inflöde Extenslam FT80101'] + 0.486474692821503\n",
    "\n",
    "# df_Nivå_Bräddning_LT23101.rename(columns={'TimeDate': 'DateTime', 'Val':'Nivå Bräddning LT23101'}, inplace=True)\n",
    "# df_Nivå_Bräddning_LT23101['DateTime'] = pd.to_datetime(df_Nivå_Bräddning_LT23101['DateTime'])\n",
    "# df_Nivå_Bräddning_LT23101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "# df_Nivå_Bräddning_LT23101.set_index('DateTime', inplace=True)\n",
    "# dprint(df_Nivå_Bräddning_LT23101.head())\n",
    "\n",
    "# df_MBBRflöde_FT30101.rename(columns={'TimeDate': 'DateTime', 'Val':'MBBRflöde FT30101'}, inplace=True)\n",
    "# df_MBBRflöde_FT30101['DateTime'] = pd.to_datetime(df_MBBRflöde_FT30101['DateTime'])\n",
    "# df_MBBRflöde_FT30101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "# df_MBBRflöde_FT30101.set_index('DateTime', inplace=True)\n",
    "# dprint(df_MBBRflöde_FT30101.head())\n",
    "\n",
    "# # Merge the DataFrames on the DateTime index, aligning values\n",
    "# # Using merge instead of concat to handle any duplicate indices\n",
    "# # Merge all DataFrames sequentially\n",
    "# df_ax = pd.merge(df_Inflöde_FT10101, df_Utflöde_FT72101, left_index=True, right_index=True, how='outer')\n",
    "# df_ax = pd.merge(df_ax, df_Inflöde_Extenslam_FT80101, left_index=True, right_index=True, how='outer')\n",
    "# df_ax = pd.merge(df_ax, df_MBBRflöde_FT30101, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# # Show the result\n",
    "# print(\"\\nMerged DataFrame:\")\n",
    "# dprint(df_ax.head())\n",
    "\n",
    "# # Check for any missing values after merge\n",
    "# print(\"\\nMissing values in merged DataFrame:\")\n",
    "# dprint(df_ax.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all column names to verify what needs renaming\n",
    "print(\"Columns in df_Inflöde_FT10101 [m3/h]:\", df_Inflöde_FT10101.columns.tolist())\n",
    "print(\"Columns in df_Utflöde_FT72101 [m3/h]:\", df_Utflöde_FT72101.columns.tolist())\n",
    "print(\"Columns in df_ax:\", df_ax.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c70e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DateTime indices where there are missing values\n",
    "print(\"\\nDateTime indices with missing FT10101:\")\n",
    "print(df_ax[df_ax['Inflöde FT10101 [m3/h]'].isna()].index.strftime('%Y-%m-%d %H:%M:%S').tolist())\n",
    "\n",
    "print(\"\\nDateTime indices with missing FT72101:\")\n",
    "print(df_ax[df_ax['Utflöde FT72101 [m3/h]'].isna()].index.strftime('%Y-%m-%d %H:%M:%S').tolist())\n",
    "\n",
    "# Print summary of gaps\n",
    "print(\"\\nSummary of gaps:\")\n",
    "print(f\"Total rows in merged DataFrame: {len(df_ax)}\")\n",
    "print(f\"Rows with missing FT10101: {df_ax['Inflöde FT10101 [m3/h]'].isna().sum()}\")\n",
    "print(f\"Rows with missing FT72101: {df_ax['Utflöde FT72101 [m3/h]'].isna().sum()}\")\n",
    "print(f\"Rows with data in both columns: {len(df_ax) - df_ax.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133f22f",
   "metadata": {},
   "source": [
    "# Check and Remove Duplicate Timestamps\n",
    "Before calculating moving averages, we need to identify and remove any duplicate timestamps from the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all duplicate timestamps before removing them\n",
    "df_duplicate_timestamps = pd.DataFrame()\n",
    "\n",
    "# Helper function to collect duplicates from a DataFrame\n",
    "def collect_duplicates(df, df_name):\n",
    "    \"\"\"Collect duplicate rows and return them with a source column.\"\"\"\n",
    "    if df.index.duplicated().any():\n",
    "        dup_mask = df.index.duplicated(keep=False)  # Mark ALL duplicates, not just subsequent ones\n",
    "        dup_rows = df[dup_mask].copy()\n",
    "        dup_rows['Source'] = df_name\n",
    "        dup_rows['DuplicateGroup'] = dup_rows.index.astype(str)\n",
    "        return dup_rows\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Check for and collect duplicate indices before removal\n",
    "frames_to_check = {\n",
    "    'df_ax': df_ax,\n",
    "    'df_Inflöde_Extenslam_FT80101': df_Inflöde_Extenslam_FT80101,\n",
    "    'df_Nivå_Bräddning_LT23101': df_Nivå_Bräddning_LT23101,\n",
    "    'df_MBBRflöde_FT30101': df_MBBRflöde_FT30101\n",
    "}\n",
    "\n",
    "duplicate_collections = []\n",
    "for name, df_frame in frames_to_check.items():\n",
    "    if df_frame.index.duplicated().any():\n",
    "        dup_count = df_frame.index.duplicated().sum()\n",
    "        print(f\"Warning: {name} has {dup_count} duplicate indices. Keeping first occurrence.\")\n",
    "        \n",
    "        # Collect duplicates\n",
    "        dup_df = collect_duplicates(df_frame, name)\n",
    "        if not dup_df.empty:\n",
    "            duplicate_collections.append(dup_df)\n",
    "        \n",
    "        # Remove duplicates from the original frame\n",
    "        if name == 'df_ax':\n",
    "            df_ax = df_ax[~df_ax.index.duplicated(keep='first')]\n",
    "        elif name == 'df_Inflöde_Extenslam_FT80101':\n",
    "            df_Inflöde_Extenslam_FT80101 = df_Inflöde_Extenslam_FT80101[~df_Inflöde_Extenslam_FT80101.index.duplicated(keep='first')]\n",
    "        elif name == 'df_Nivå_Bräddning_LT23101':\n",
    "            df_Nivå_Bräddning_LT23101 = df_Nivå_Bräddning_LT23101[~df_Nivå_Bräddning_LT23101.index.duplicated(keep='first')]\n",
    "        elif name == 'df_MBBRflöde_FT30101':\n",
    "            df_MBBRflöde_FT30101 = df_MBBRflöde_FT30101[~df_MBBRflöde_FT30101.index.duplicated(keep='first')]\n",
    "\n",
    "# Combine all duplicate collections into one DataFrame\n",
    "if duplicate_collections:\n",
    "    df_duplicate_timestamps = pd.concat(duplicate_collections, axis=0)\n",
    "    df_duplicate_timestamps = df_duplicate_timestamps.sort_values(['DuplicateGroup', 'Source'])\n",
    "    print(f\"\\nTotal duplicate rows collected: {len(df_duplicate_timestamps)}\")\n",
    "    print(f\"Unique duplicate timestamps: {df_duplicate_timestamps.index.nunique()}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = 'duplicate_timestamps.csv'\n",
    "    df_duplicate_timestamps.to_csv(csv_path)\n",
    "    print(f\"Duplicate timestamps saved to: {csv_path}\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates found in any DataFrame.\")\n",
    "\n",
    "print(f\"\\nCleaned DataFrame sizes:\")\n",
    "print(f\"  df_ax: {len(df_ax)} rows\")\n",
    "print(f\"  df_Inflöde_Extenslam_FT80101: {len(df_Inflöde_Extenslam_FT80101)} rows\")\n",
    "print(f\"  df_Nivå_Bräddning_LT23101: {len(df_Nivå_Bräddning_LT23101)} rows\")\n",
    "print(f\"  df_MBBRflöde_FT30101: {len(df_MBBRflöde_FT30101)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9be0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of duplicate timestamps\n",
    "if not df_duplicate_timestamps.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DUPLICATE TIMESTAMPS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total duplicate rows captured: {len(df_duplicate_timestamps)}\")\n",
    "    print(f\"Number of unique duplicate timestamps: {df_duplicate_timestamps.index.nunique()}\")\n",
    "    \n",
    "    print(\"\\nDuplicates by source DataFrame:\")\n",
    "    source_counts = df_duplicate_timestamps.groupby('Source').size()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} rows\")\n",
    "    \n",
    "    print(\"\\nUnique timestamps that appear multiple times:\")\n",
    "    unique_dups = sorted(df_duplicate_timestamps.index.unique())\n",
    "    for ts in unique_dups:\n",
    "        print(f\"  {ts.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Show a sample of the data for one duplicate timestamp\n",
    "    print(\"\\nExample: Data for first duplicate timestamp\")\n",
    "    first_dup = unique_dups[0]\n",
    "    sample = df_duplicate_timestamps[df_duplicate_timestamps.index == first_dup]\n",
    "    print(sample.to_string())\n",
    "    \n",
    "    # Optionally save to CSV\n",
    "    csv_path = 'duplicate_timestamps.csv'\n",
    "    df_duplicate_timestamps.to_csv(csv_path)\n",
    "    print(f\"\\nDuplicate timestamps saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ed4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display duplicate timestamps if any were found\n",
    "if 'df_duplicate_timestamps' in locals() and not df_duplicate_timestamps.empty:\n",
    "    print(f\"Duplicate timestamps DataFrame shape: {df_duplicate_timestamps.shape}\")\n",
    "    print(\"\\nFirst few duplicate rows:\")\n",
    "    print(df_duplicate_timestamps.head(20))\n",
    "    \n",
    "    # Show summary by source\n",
    "    print(\"\\nDuplicates by source:\")\n",
    "    print(df_duplicate_timestamps.groupby('Source').size())\n",
    "    \n",
    "    # Show the actual duplicate timestamps\n",
    "    print(\"\\nUnique duplicate timestamps:\")\n",
    "    print(sorted(df_duplicate_timestamps.index.unique().strftime('%Y-%m-%d %H:%M:%S').tolist()))\n",
    "else:\n",
    "    print(\"No duplicates were found or df_duplicate_timestamps is empty.\")\n",
    "    print(\"\\nNote: If you previously had duplicates, you may need to:\")\n",
    "    print(\"1. Re-run the data loading cells (cells 4-5)\")\n",
    "    print(\"2. Then re-run the moving averages cell to capture duplicates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3605581",
   "metadata": {},
   "source": [
    "# Calculate Velocity for FT10101\n",
    "Convert flow rate (m³/s) to velocity (m/s) using pipe inside diameter of 300 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pipe inside diameter in meters\n",
    "diameter_m = 0.300  # 300 mm = 0.3 m\n",
    "\n",
    "# Calculate cross-sectional area: A = π × (d/2)²\n",
    "area_m2 = np.pi * (diameter_m / 2) ** 2\n",
    "\n",
    "print(f\"Pipe inside diameter: {diameter_m * 1000} mm\")\n",
    "print(f\"Cross-sectional area: {area_m2:.6f} m²\")\n",
    "\n",
    "# Calculate velocity: v = Q / A\n",
    "# df_ax['Inflöde FT10101'] is in m³/s (assuming flow rate units)\n",
    "# Velocity will be in m/s\n",
    "df_Inflöde_FT10101_mps = pd.DataFrame(index=df_ax.index)\n",
    "df_Inflöde_FT10101_mps['Inflöde FT10101 [m/s]'] = df_ax['Inflöde FT10101 [m3/h]'] / 3.6 / 1000 / area_m2\n",
    "\n",
    "print(f\"\\nVelocity statistics:\")\n",
    "print(df_Inflöde_FT10101_mps['Inflöde FT10101 [m/s]'].describe())\n",
    "print(f\"\\nFirst few values:\")\n",
    "print(df_Inflöde_FT10101_mps.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b8cf3",
   "metadata": {},
   "source": [
    "# Calculate Moving Averages for Velocity\n",
    "Apply the same time-based moving average windows (1h, 24h, 7d) to the velocity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7319c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages for velocity data\n",
    "# Prepare container for velocity moving averages\n",
    "df_velocity_ma = pd.DataFrame(index=df_Inflöde_FT10101_mps.index)\n",
    "\n",
    "# Define the same time-based windows as used for flows\n",
    "#windows_velocity = { '12min': '12min', '1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "windows_velocity = {'1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "\n",
    "for col in df_Inflöde_FT10101_mps.columns:\n",
    "    for w_label, w_offset in windows_velocity.items():\n",
    "        # Use time-based rolling which is robust to missing/irregular timestamps\n",
    "        ma = df_Inflöde_FT10101_mps[col].rolling(w_offset, min_periods=1).mean()\n",
    "        ma_col_name = f\"{col}_MA_{w_label}\"\n",
    "        df_velocity_ma[ma_col_name] = ma\n",
    "\n",
    "# Concat original velocity data with its moving averages\n",
    "df_velocity_with_ma = pd.concat([df_Inflöde_FT10101_mps, df_velocity_ma], axis=1)\n",
    "\n",
    "print(\"Velocity DataFrame with Moving Averages:\")\n",
    "print(f\"Shape: {df_velocity_with_ma.shape}\")\n",
    "print(f\"Columns: {df_velocity_with_ma.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_velocity_with_ma.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a4f70",
   "metadata": {},
   "source": [
    "# Compute Overflow Rate from Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overflow rate from level df_Nivå_Bräddning_LT23101 -> df_Utflöde_Bräddning_LT23101\n",
    "import numpy as np\n",
    "\n",
    "# Constants for the V-notch weir formula (Excel equivalent):\n",
    "# =IF(E2<0.25;0;0.58*8/15*TAN(RADIANS(100)/2) * (E2-H)^(2.5) * SQRT(2*g) * 3600)\n",
    "H_threshold = 0.25  # m, crest level H in the formula\n",
    "angle_deg = 100.0   # degrees for the V-notch angle\n",
    "g = 9.81            # m/s^2\n",
    "df_Tröskelnivå_Bräddning_LT23101 = pd.DataFrame(\n",
    "    data={'Tröskelnivå Bräddning LT23101 [m]': H_threshold},\n",
    "    index=df_Nivå_Bräddning_LT23101.index\n",
    ")\n",
    "\n",
    "print(df_Nivå_Bräddning_LT23101['Nivå Bräddning LT23101 [m]'].describe())\n",
    "# Prepare head above crest (clipped at 0)\n",
    "head = (df_Nivå_Bräddning_LT23101['Nivå Bräddning LT23101 [m]'] - H_threshold).clip(lower=0.0)\n",
    "\n",
    "\n",
    "# Precompute constant multiplier K = 0.58*8/15*TAN(RADIANS(100)/2)*SQRT(2*g)*3600\n",
    "K = 0.58 * (8.0/15.0) * np.tan(np.radians(angle_deg)/2.0) * np.sqrt(2.0 * g) * 3600.0\n",
    "\n",
    "# Flow [m3/h]\n",
    "flow_m3h = K * np.power(head, 2.5)\n",
    "\n",
    "# Build resulting DataFrame\n",
    "df_Utflöde_Bräddning_LT23101 = pd.DataFrame(\n",
    "    data={'Utflöde Bräddning LT23101 [m3/h]': flow_m3h},\n",
    "    index=df_Nivå_Bräddning_LT23101.index\n",
    ")\n",
    "\n",
    "# Check for and remove duplicates in the overflow DataFrame\n",
    "if df_Utflöde_Bräddning_LT23101.index.duplicated().any():\n",
    "    dup_count = df_Utflöde_Bräddning_LT23101.index.duplicated().sum()\n",
    "    print(f\"Warning: df_Utflöde_Bräddning_LT23101 [m3/h] has {dup_count} duplicate indices. Keeping first occurrence.\")\n",
    "    df_Utflöde_Bräddning_LT23101 = df_Utflöde_Bräddning_LT23101[~df_Utflöde_Bräddning_LT23101.index.duplicated(keep='first')]\n",
    "\n",
    "# Merge overflow flow into df_ax immediately so it's available for subsequent cells\n",
    "df_ax = pd.merge(df_ax, df_Utflöde_Bräddning_LT23101, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Optional diagnostics\n",
    "print('Computed df_Utflöde_Bräddning_LT23101 [m3/h]:')\n",
    "dprint(df_Utflöde_Bräddning_LT23101.head())\n",
    "print(f'df_Utflöde_Bräddning_LT23101 [m3/h]: {len(df_Utflöde_Bräddning_LT23101)} rows')\n",
    "print(f'Merged into df_ax. df_ax now has {len(df_ax.columns)} columns.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae27ee",
   "metadata": {},
   "source": [
    "# Understanding pd.NA vs np.nan\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "### `np.nan` (NumPy NaN)\n",
    "- **Type**: `float` - it's literally the floating-point value \"Not a Number\"\n",
    "- **Data Type**: Always has dtype `float64`\n",
    "- **Compatibility**: Works with **matplotlib**, NumPy, and most numeric operations\n",
    "- **Use Case**: Traditional missing value representation for numeric data\n",
    "- **Example**: `np.nan + 5` returns `np.nan` (propagates through calculations)\n",
    "\n",
    "### `pd.NA` (Pandas NA)\n",
    "- **Type**: Special pandas object of type `NAType`\n",
    "- **Data Type**: Can work with nullable integer, boolean, and string types (e.g., `Int64`, `boolean`)\n",
    "- **Compatibility**: **NOT compatible with matplotlib** - causes `TypeError` when matplotlib tries to convert to float\n",
    "- **Use Case**: Modern pandas nullable data types that distinguish \"missing\" from \"NaN\"\n",
    "- **Example**: `pd.NA + 5` returns `pd.NA` (also propagates)\n",
    "\n",
    "## Why We Use np.nan for Plotting\n",
    "\n",
    "When passing data to matplotlib for visualization:\n",
    "- Matplotlib internally converts arrays to float using `np.asarray(x, float)`\n",
    "- `np.nan` is already a float → ✅ Works perfectly\n",
    "- `pd.NA` is a special object → ❌ Raises `TypeError: float() argument must be a string or a real number, not 'NAType'`\n",
    "\n",
    "## Best Practice\n",
    "- Use `np.nan` for numeric DataFrames that will be plotted\n",
    "- Use `pd.NA` for nullable integer/boolean columns where you need to distinguish between missing and zero\n",
    "- **Always convert to np.nan before plotting**: `df = df.fillna(np.nan)`\n",
    "\n",
    "## Example from This Notebook\n",
    "```python\n",
    "# Before fix: caused TypeError in matplotlib\n",
    "df_ma_diff[diff_col] = pd.NA  # ❌ Breaks plotting\n",
    "\n",
    "# After fix: works with matplotlib\n",
    "df_ma_diff[diff_col] = np.nan  # ✅ Plots successfully\n",
    "\n",
    "# Failsafe conversion before any plotting\n",
    "df_flows = df_flows.fillna(np.nan)\n",
    "df_flowdiff = df_flowdiff.fillna(np.nan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249e445",
   "metadata": {},
   "source": [
    "# Overflow Level Interactive Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff04900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "df_Nivå_och_Tröskelnivå_Bräddning_LT23101 = pd.merge(df_Nivå_Bräddning_LT23101,df_Tröskelnivå_Bräddning_LT23101, \n",
    "                     left_index=True, right_index=True, how='outer')\n",
    "if False:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance() or QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series = len(df_Nivå_och_Tröskelnivå_Bräddning_LT23101.columns) + len(df_Utflöde_Bräddning_LT23101.columns)\n",
    "            settings_key = f'Flöden och Flödesdiff ({n_series} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "            \n",
    "            mainWin = IPW(df_axL = df_Nivå_och_Tröskelnivå_Bräddning_LT23101,\n",
    "                                df_axL_Title = 'Nivå [m]', \n",
    "                                df_axR = df_Utflöde_Bräddning_LT23101, \n",
    "                                df_axR_Title = 'Bräddnivåflöde [m3/h]',\n",
    "                                WindowTitle='Pajala ARV Bräddning - Nivå och Flöde',\n",
    "                                settings_file=f'InteractivePlotWindow.json::{settings_key}'\n",
    "                            )\n",
    "            mainWin.show()\n",
    "            # Keep references to avoid garbage collection in notebook kernels.\n",
    "            \n",
    "            # Store on the app and module globals so the objects persist after this function returns.\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "            app = _make_and_show()\n",
    "            # Keep references in the IPython user namespace if available so users can interact with them\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    # Fall back to module globals (already set by _make_and_show)\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: start the blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab554858",
   "metadata": {},
   "source": [
    "# Interpolate Missing Values in df_ax\n",
    "Create a new DataFrame with interpolated values to fill gaps in the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ee366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectively replace zeros with NaN (so zeros are treated as missing and interpolated)\n",
    "# Run this BEFORE the interpolation cell. Then re-run the interpolation cell to apply.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Choose which columns should treat 0 as missing\n",
    "#    Edit this list to match the columns where zero means \"no reading\" (not a real zero)\n",
    "columns_to_zero_to_nan = [\n",
    "    'Inflöde FT10101 [m3/h]',\n",
    "    'Utflöde FT72101 [m3/h]',\n",
    "    'MBBRflöde FT30101 [m3/h]'\n",
    "]\n",
    "# 'Utflöde Bräddning LT23101' and 'Inflöde Extenslam FT80101' should contain zero-values legitimately, so we do NOT include it here\n",
    "\n",
    "# Keep only those that exist and are numeric\n",
    "target_cols = [c for c in columns_to_zero_to_nan if c in df_ax.columns and pd.api.types.is_numeric_dtype(df_ax[c])]\n",
    "if not target_cols:\n",
    "    print(\"No matching numeric columns found in df_ax for zero→NaN replacement.\")\n",
    "else:\n",
    "    # 2) Optional: treat near-zero as zero (set to >0 for tolerance)\n",
    "    zero_eps = 0.0  # e.g., 1e-6 to also treat tiny magnitudes as zero\n",
    "\n",
    "    print(\"Replacing zeros with NaN in columns:\", target_cols)\n",
    "    total_replaced = 0\n",
    "    for col in target_cols:\n",
    "        s = df_ax[col]\n",
    "        mask = (s == 0) if zero_eps == 0 else (s.abs() <= zero_eps)\n",
    "        n = int(mask.sum())\n",
    "        if n > 0:\n",
    "            df_ax.loc[mask, col] = np.nan\n",
    "        print(f\"  {col}: replaced {n} value(s)\")\n",
    "        total_replaced += n\n",
    "\n",
    "    print(f\"Total replacements: {total_replaced}\")\n",
    "    print(\"Now re-run the interpolation cell to include these NaNs in interpolation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpolated version of df_ax to fill missing values\n",
    "# 1) Expand the time index to include missing timestamps\n",
    "# 2) Interpolate interior gaps with SMOOTHING (windowed-average endpoints + linear)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure a sorted, unique DateTime index for reliable reindexing\n",
    "_df_sorted = df_ax.sort_index()\n",
    "if _df_sorted.index.has_duplicates:\n",
    "    _df_sorted = _df_sorted[~_df_sorted.index.duplicated(keep='first')]\n",
    "\n",
    "# Infer frequency; fall back to 1 minute if unknown\n",
    "_inferred_freq = pd.infer_freq(_df_sorted.index)\n",
    "if _inferred_freq is None:\n",
    "    _inferred_freq = '1min'\n",
    "    print(\"Note: Could not infer index frequency. Assuming 1 minute.\")\n",
    "\n",
    "# Build complete time range and reindex to insert fully-missing timestamp rows\n",
    "_full_index = pd.date_range(_df_sorted.index.min(), _df_sorted.index.max(), freq=_inferred_freq)\n",
    "df_ax_full = _df_sorted.reindex(_full_index)\n",
    "\n",
    "# Preserve the index name (e.g., 'DateTime') so downstream code and displays keep the heading\n",
    "_idx_name = _df_sorted.index.name or 'DateTime'\n",
    "df_ax_full.index.name = _idx_name\n",
    "\n",
    "print(f\"Inserted missing timestamps: {len(_full_index) - len(_df_sorted.index)}\")\n",
    "print(f\"Full index length: {len(_full_index)} (from {_full_index.min()} to {_full_index.max()}, freq={_inferred_freq})\")\n",
    "print(f\"Index name: {df_ax_full.index.name}\")\n",
    "\n",
    "# Copy to preserve a separate interpolated frame\n",
    "df_ax_interpolated = df_ax_full.copy()\n",
    "# Ensure the copy also carries the index name explicitly\n",
    "df_ax_interpolated.index.name = _idx_name\n",
    "\n",
    "# Report missing values before interpolation\n",
    "print(\"\\nMissing values BEFORE interpolation (after reindex):\")\n",
    "print(df_ax_interpolated.isna().sum())\n",
    "print(f\"\\nTotal NaNs: {df_ax_interpolated.isna().sum().sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SMOOTHED INTERPOLATION: Windowed-Average Endpoints + Linear\n",
    "# ============================================================================\n",
    "# Instead of connecting adjacent points, this method:\n",
    "# 1. Finds each gap (sequence of NaN values)\n",
    "# 2. Calculates mean of N minutes BEFORE the gap (start value)\n",
    "# 3. Calculates mean of N minutes AFTER the gap (end value)\n",
    "# 4. Linearly interpolates between these averaged endpoints\n",
    "#\n",
    "# This smooths out noise while preserving trends.\n",
    "\n",
    "# Configuration\n",
    "averaging_window = '15min'  # Window size for averaging before/after gaps\n",
    "use_windowed_interpolation = True  # Set False to use standard methods below\n",
    "\n",
    "print(f\"\\nInterpolation method: Windowed-Average Linear (window={averaging_window})\")\n",
    "\n",
    "if use_windowed_interpolation:\n",
    "    _numeric_cols = [c for c in df_ax_interpolated.columns if pd.api.types.is_numeric_dtype(df_ax_interpolated[c])]\n",
    "    \n",
    "    for col in _numeric_cols:\n",
    "        s = df_ax_interpolated[col].copy()\n",
    "        \n",
    "        # Identify valid data boundaries (for limit_area='inside')\n",
    "        first_valid = s.first_valid_index()\n",
    "        last_valid = s.last_valid_index()\n",
    "        \n",
    "        if first_valid is None or last_valid is None:\n",
    "            continue  # Column is all NaN\n",
    "        \n",
    "        # Only interpolate within valid data range (limit_area='inside')\n",
    "        interior_mask = (s.index >= first_valid) & (s.index <= last_valid)\n",
    "        s_interior = s[interior_mask]\n",
    "        \n",
    "        # Find gaps (consecutive NaN sequences)\n",
    "        is_nan = s_interior.isna()\n",
    "        gap_starts = is_nan & ~is_nan.shift(1, fill_value=False)\n",
    "        gap_ends = is_nan & ~is_nan.shift(-1, fill_value=False)\n",
    "        \n",
    "        gap_start_indices = s_interior.index[gap_starts]\n",
    "        gap_end_indices = s_interior.index[gap_ends]\n",
    "        \n",
    "        # Process each gap\n",
    "        for gap_start, gap_end in zip(gap_start_indices, gap_end_indices):\n",
    "            # Get indices for averaging windows\n",
    "            gap_start_loc = s_interior.index.get_loc(gap_start)\n",
    "            gap_end_loc = s_interior.index.get_loc(gap_end)\n",
    "            \n",
    "            # Calculate time window for averaging\n",
    "            try:\n",
    "                window_offset = pd.Timedelta(averaging_window)\n",
    "            except Exception:\n",
    "                window_offset = pd.Timedelta(minutes=15)\n",
    "            \n",
    "            # BEFORE gap: average of data from (gap_start - window) to gap_start\n",
    "            before_window_start = gap_start - window_offset\n",
    "            before_data = s_interior[(s_interior.index >= before_window_start) & (s_interior.index < gap_start)]\n",
    "            before_avg = before_data.mean() if len(before_data) > 0 and not before_data.isna().all() else None\n",
    "            \n",
    "            # AFTER gap: average of data from gap_end to (gap_end + window)\n",
    "            after_window_end = gap_end + window_offset\n",
    "            after_data = s_interior[(s_interior.index > gap_end) & (s_interior.index <= after_window_end)]\n",
    "            after_avg = after_data.mean() if len(after_data) > 0 and not after_data.isna().all() else None\n",
    "            \n",
    "            # If we have both averaged endpoints, interpolate linearly between them\n",
    "            if before_avg is not None and after_avg is not None:\n",
    "                # Get all indices in the gap (including boundaries)\n",
    "                gap_indices = s_interior.index[(s_interior.index >= gap_start) & (s_interior.index <= gap_end)]\n",
    "                n_points = len(gap_indices)\n",
    "                \n",
    "                if n_points > 0:\n",
    "                    # Create linear interpolation between averaged endpoints\n",
    "                    interpolated_values = np.linspace(before_avg, after_avg, n_points + 2)[1:-1]\n",
    "                    s_interior.loc[gap_indices] = interpolated_values\n",
    "        \n",
    "        # Update the main dataframe with interpolated values\n",
    "        df_ax_interpolated.loc[interior_mask, col] = s_interior\n",
    "        \n",
    "    print(f\"Applied windowed-average interpolation to {len(_numeric_cols)} columns\")\n",
    "    \n",
    "else:\n",
    "    # ALTERNATIVE STANDARD METHODS (if windowed interpolation is disabled)\n",
    "    # Uncomment one of these:\n",
    "    \n",
    "    # Option 1: Cubic spline (smooth curves, good for most time series)\n",
    "    interpolation_method = 'cubic'\n",
    "    interpolation_order = None\n",
    "    \n",
    "    # Option 2: Polynomial (smoother, may overshoot)\n",
    "    # interpolation_method = 'polynomial'\n",
    "    # interpolation_order = 2  # 2 or 3 recommended\n",
    "    \n",
    "    # Option 3: Akima spline (smooth, less overshoot than cubic)\n",
    "    # interpolation_method = 'akima'\n",
    "    # interpolation_order = None\n",
    "    \n",
    "    # Option 4: Standard linear (point-to-point)\n",
    "    # interpolation_method = 'linear'\n",
    "    # interpolation_order = None\n",
    "    \n",
    "    print(f\"\\nUsing standard interpolation method: {interpolation_method}\" + \n",
    "          (f\" (order={interpolation_order})\" if interpolation_order else \"\"))\n",
    "    \n",
    "    _numeric_cols = [c for c in df_ax_interpolated.columns if pd.api.types.is_numeric_dtype(df_ax_interpolated[c])]\n",
    "    \n",
    "    for col in _numeric_cols:\n",
    "        try:\n",
    "            if interpolation_method == 'polynomial' and interpolation_order:\n",
    "                df_ax_interpolated[col] = df_ax_interpolated[col].interpolate(\n",
    "                    method=interpolation_method,\n",
    "                    order=interpolation_order,\n",
    "                    limit_area='inside'\n",
    "                )\n",
    "            else:\n",
    "                df_ax_interpolated[col] = df_ax_interpolated[col].interpolate(\n",
    "                    method=interpolation_method,\n",
    "                    limit_area='inside'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not use {interpolation_method} for {col}, falling back to linear. Error: {e}\")\n",
    "            df_ax_interpolated[col] = df_ax_interpolated[col].interpolate(\n",
    "                method='linear',\n",
    "                limit_area='inside'\n",
    "            )\n",
    "\n",
    "# Report missing values after interpolation\n",
    "print(\"\\nMissing values AFTER interpolation:\")\n",
    "print(df_ax_interpolated.isna().sum())\n",
    "print(f\"\\nTotal NaNs: {df_ax_interpolated.isna().sum().sum()}\")\n",
    "\n",
    "# Show some examples where interpolation filled gaps\n",
    "print(\"\\nExample: First few rows of original vs full vs interpolated data\")\n",
    "print(\"\\nOriginal df_ax (head):\")\n",
    "print(df_ax.head(10))\n",
    "print(\"\\nReindexed df_ax_full (head):\")\n",
    "print(df_ax_full.head(10))\n",
    "print(\"\\nInterpolated df_ax_interpolated (head):\")\n",
    "print(df_ax_interpolated.head(10))\n",
    "\n",
    "# --- Additional diagnostics requested previously: print rows with missing data (original df_ax) ---\n",
    "print(\"\\nRows with any missing values (original df_ax):\")\n",
    "rows_with_any_nan = df_ax[df_ax.isna().any(axis=1)]\n",
    "print(f\"Total rows with >=1 NaN: {len(rows_with_any_nan)} out of {len(df_ax)}\")\n",
    "print(\"Showing first 30 rows with NaN (use the DataFrame variable to inspect more):\")\n",
    "try:\n",
    "    print(rows_with_any_nan.head(30).to_string())\n",
    "except Exception:\n",
    "    print(rows_with_any_nan.head(30))\n",
    "\n",
    "# Identify interior (non-edge) NaNs per column on the original df_ax (informational)\n",
    "interior_nan_rows = set()\n",
    "interior_counts = {}\n",
    "for col in df_ax.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df_ax[col]):\n",
    "        continue\n",
    "    if not df_ax[col].isna().any():\n",
    "        continue\n",
    "    first_valid = df_ax[col].first_valid_index()\n",
    "    last_valid = df_ax[col].last_valid_index()\n",
    "    if first_valid is None or last_valid is None:\n",
    "        continue\n",
    "    edge_mask = (df_ax.index < first_valid) | (df_ax.index > last_valid)\n",
    "    interior_mask = (~edge_mask) & df_ax[col].isna()\n",
    "    count_interior = int(interior_mask.sum())\n",
    "    if count_interior > 0:\n",
    "        interior_nan_rows.update(df_ax.index[interior_mask].tolist())\n",
    "    interior_counts[col] = count_interior\n",
    "\n",
    "print(\"\\nInterior NaN rows (within valid span for each column):\", len(interior_nan_rows))\n",
    "if interior_nan_rows:\n",
    "    sample_idx = sorted(list(interior_nan_rows))[:50]\n",
    "    print(\"Showing up to 50 interior-NaN rows:\")\n",
    "    try:\n",
    "        print(df_ax.loc[sample_idx].to_string())\n",
    "    except Exception:\n",
    "        print(df_ax.loc[sample_idx])\n",
    "else:\n",
    "    print(\"No interior NaN rows detected; NaNs appear at edges for their respective columns.\")\n",
    "\n",
    "print(\"\\nInterior NaN counts per numeric column:\")\n",
    "for col, cnt in interior_counts.items():\n",
    "    print(f\"  {col}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge NaN summary after interpolation\n",
    "import pandas as pd\n",
    "\n",
    "# Require df_ax_interpolated created in interpolation cell\n",
    "if 'df_ax_interpolated' not in globals():\n",
    "    print(\"df_ax_interpolated is not defined. Run the interpolation cell first.\")\n",
    "else:\n",
    "    leading_counts = {}\n",
    "    trailing_counts = {}\n",
    "    interior_remaining_counts = {}\n",
    "    for col in df_ax_interpolated.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df_ax_interpolated[col]):\n",
    "            continue\n",
    "        s = df_ax_interpolated[col]\n",
    "        if not s.isna().any():\n",
    "            leading_counts[col] = 0\n",
    "            trailing_counts[col] = 0\n",
    "            interior_remaining_counts[col] = 0\n",
    "            continue\n",
    "        first_valid = s.first_valid_index()\n",
    "        last_valid = s.last_valid_index()\n",
    "        # If no valid data at all\n",
    "        if first_valid is None or last_valid is None:\n",
    "            leading_counts[col] = int(s.isna().sum())\n",
    "            trailing_counts[col] = 0\n",
    "            interior_remaining_counts[col] = 0\n",
    "            continue\n",
    "        leading_mask = (df_ax_interpolated.index < first_valid)\n",
    "        trailing_mask = (df_ax_interpolated.index > last_valid)\n",
    "        interior_mask = (~leading_mask) & (~trailing_mask)\n",
    "        leading_counts[col] = int(s[leading_mask].isna().sum())\n",
    "        trailing_counts[col] = int(s[trailing_mask].isna().sum())\n",
    "        # Any NaNs that remain interior (should be zero with limit_area='inside')\n",
    "        interior_remaining_counts[col] = int(s[interior_mask].isna().sum())\n",
    "\n",
    "    print(\"Edge NaN Summary (post-interpolation):\")\n",
    "    print(\"Column | Leading NaNs | Trailing NaNs | Interior NaNs (remaining)\")\n",
    "    for col in leading_counts.keys():\n",
    "        print(f\"{col} | {leading_counts[col]} | {trailing_counts[col]} | {interior_remaining_counts[col]}\")\n",
    "\n",
    "    total_leading = sum(leading_counts.values())\n",
    "    total_trailing = sum(trailing_counts.values())\n",
    "    total_interior_remain = sum(interior_remaining_counts.values())\n",
    "    print(\"\\nTotals:\")\n",
    "    print(f\"Leading edge NaNs: {total_leading}\")\n",
    "    print(f\"Trailing edge NaNs: {total_trailing}\")\n",
    "    print(f\"Interior remaining NaNs: {total_interior_remain}\")\n",
    "\n",
    "    # Percentages relative to all NaNs left\n",
    "    total_remaining = total_leading + total_trailing + total_interior_remain\n",
    "    if total_remaining > 0:\n",
    "        pct_leading = 100 * total_leading / total_remaining\n",
    "        pct_trailing = 100 * total_trailing / total_remaining\n",
    "        pct_interior = 100 * total_interior_remain / total_remaining\n",
    "        print(f\"\\nPercent distribution of remaining NaNs:\")\n",
    "        print(f\"Leading: {pct_leading:.2f}% | Trailing: {pct_trailing:.2f}% | Interior: {pct_interior:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo NaNs remain after interpolation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51d078",
   "metadata": {},
   "source": [
    "# Calculate Moving Averages and Flow Plant Balance\n",
    "Now that duplicates have been removed, we can safely calculate moving averages with different window sizes to smooth the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ax = df_ax_interpolated.copy()\n",
    "print(f\"df_ax after interpolation has {len(df_ax)} rows and {len(df_ax.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total inflow and outflow\n",
    "df_ax['Total Inflöde [m3/h]'] = df_ax['Inflöde FT10101 [m3/h]'] + df_ax['Inflöde Extenslam FT80101 [m3/h]']\n",
    "df_ax['Total Utflöde [m3/h]'] = df_ax['Utflöde FT72101 [m3/h]'] + df_ax['Utflöde Bräddning LT23101 [m3/h]']\n",
    "dprint(df_ax[['Total Inflöde [m3/h]', 'Total Utflöde [m3/h]']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29672c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages for each column (time-based windows)\n",
    "# Each row in df_ax represents 1 minute, so use time-based rolling windows\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Note: df_Utflöde_Bräddning_LT23101 was already merged into df_ax in the overflow calculation cell\n",
    "\n",
    "# Prepare container for moving averages\n",
    "df_ma = pd.DataFrame(index=df_ax.index)\n",
    "\n",
    "# Define time-based windows (labels -> pandas offset strings)\n",
    "#windows = { '12min': '12min', '1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "windows = { '1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "\n",
    "# Only compute MAs for numeric columns\n",
    "numeric_cols = [c for c in df_ax.columns if pd.api.types.is_numeric_dtype(df_ax[c])]\n",
    "for col in numeric_cols:\n",
    "    for w_label, w_offset in windows.items():\n",
    "        ma = df_ax[col].rolling(w_offset, min_periods=1).mean()\n",
    "        df_ma[f\"{col}_MA_{w_label}\"] = ma\n",
    "\n",
    "# Compute differences (Inflöde - Utflöde - Bräddning) for raw data and each moving-average window\n",
    "# Source column names in df_ax are 'Inflöde FT10101', 'Utflöde FT72101', and 'Utflöde Bräddning LT23101'\n",
    "df_ma_diff = pd.DataFrame(index=df_ax.index)\n",
    "inflow_main = 'Inflöde FT10101 [m3/h]'\n",
    "outflow_main = 'Utflöde FT72101 [m3/h]'\n",
    "inflow_externslam = 'Inflöde Extenslam FT80101 [m3/h]'\n",
    "outflow_bräddning = 'Utflöde Bräddning LT23101 [m3/h]'\n",
    "\n",
    "required_raw = [inflow_main, outflow_main, inflow_externslam, outflow_bräddning]\n",
    "missing_raw = [c for c in required_raw if c not in df_ax.columns]\n",
    "\n",
    "# Raw instantaneous ratio (Diff_1min)\n",
    "if not missing_raw:\n",
    "    total_inflow_raw = df_ax[inflow_main] + df_ax[inflow_externslam]\n",
    "    total_outflow_raw = df_ax[outflow_main] + df_ax[outflow_bräddning]\n",
    "    denom_raw = total_inflow_raw.replace(0, np.nan)\n",
    "    df_diff_raw = (total_inflow_raw - total_outflow_raw) / denom_raw\n",
    "    df_diff_raw.name = 'Diff_1min'\n",
    "else:\n",
    "    df_diff_raw = pd.Series(data=np.nan, index=df_ax.index, name='Diff_1min')\n",
    "    print(f\"Warning: cannot compute Diff_1min, missing columns: {missing_raw}\")\n",
    "\n",
    "# Moving-average based ratios\n",
    "missing_ma_warnings = []\n",
    "for w_label in windows.keys():\n",
    "    col_inflow_main = f\"{inflow_main}_MA_{w_label}\"\n",
    "    col_outflow_main = f\"{outflow_main}_MA_{w_label}\"\n",
    "    col_inflow_externslam = f\"{inflow_externslam}_MA_{w_label}\"\n",
    "    col_outflow_bräddning = f\"{outflow_bräddning}_MA_{w_label}\"\n",
    "    needed = [col_inflow_main, col_outflow_main, col_inflow_externslam, col_outflow_bräddning]\n",
    "    if all(c in df_ma.columns for c in needed):\n",
    "        total_inflow_ma = df_ma[col_inflow_main] + df_ma[col_inflow_externslam]\n",
    "        total_outflow_ma = df_ma[col_outflow_main] + df_ma[col_outflow_bräddning]\n",
    "        denom_ma = total_inflow_ma.replace(0, np.nan)\n",
    "        df_ma_diff[f\"Diff_MA_{w_label}\"] = (total_inflow_ma - total_outflow_ma) / denom_ma\n",
    "    else:\n",
    "        df_ma_diff[f\"Diff_MA_{w_label}\"] = np.nan\n",
    "        missing_set = [c for c in needed if c not in df_ma.columns]\n",
    "        missing_ma_warnings.append((w_label, missing_set))\n",
    "\n",
    "if missing_ma_warnings:\n",
    "    for w_label, miss in missing_ma_warnings:\n",
    "        print(f\"Warning: cannot compute Diff_MA_{w_label} because missing MA columns: {miss}\")\n",
    "\n",
    "# Concat original df_ax and its moving averages (left axis)\n",
    "df_ax_ma = pd.concat([df_ax, df_ma], axis=1)\n",
    "\n",
    "# Right axis: raw diff plus MA diffs\n",
    "df_flowdiff = pd.concat([df_diff_raw.to_frame(), df_ma_diff], axis=1)\n",
    "\n",
    "# CRITICAL: Convert any remaining pd.NA to np.nan for matplotlib compatibility\n",
    "df_ax_ma = df_ax_ma.fillna(np.nan)\n",
    "df_flowdiff = df_flowdiff.fillna(np.nan)\n",
    "\n",
    "# Align left and right frames to a common union index (sorted)\n",
    "union_idx = df_ax_ma.index.union(df_flowdiff.index)\n",
    "try:\n",
    "    union_idx = union_idx.unique()\n",
    "except Exception:\n",
    "    pass\n",
    "union_idx = union_idx.sort_values()\n",
    "\n",
    "df_ax_ma = df_ax_ma.reindex(union_idx)\n",
    "df_flowdiff = df_flowdiff.reindex(union_idx)\n",
    "\n",
    "# Optional: sanity prints\n",
    "print(\"Aligned lengths (L, R):\", len(df_ax_ma.index), len(df_flowdiff.index))\n",
    "print(\"Left NaNs total:\", int(df_ax_ma.isna().sum().sum()))\n",
    "print(\"Right NaNs total:\", int(df_flowdiff.isna().sum().sum()))\n",
    "print(\"Diff columns:\", df_flowdiff.columns.tolist())\n",
    "print(\"Non-NaN counts per diff column:\\n\", df_flowdiff.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder df_flows columns so MA columns follow their corresponding raw data columns\n",
    "# Strategy: For each raw column in df_ax, add it and then add all its MA variants\n",
    "\n",
    "# Get list of original df_ax columns (before MA calculation)\n",
    "raw_columns = [col for col in df_ax_ma.columns if '_MA_' not in col]\n",
    "\n",
    "# Build ordered column list: for each raw column, add it plus all its MA variants\n",
    "ordered_columns = []\n",
    "for raw_col in raw_columns:\n",
    "    # Add the raw column\n",
    "    ordered_columns.append(raw_col)\n",
    "    # Add all MA columns for this raw column\n",
    "    for w_label in windows.keys():\n",
    "        ma_col = f\"{raw_col}_MA_{w_label}\"\n",
    "        if ma_col in df_ax_ma.columns:\n",
    "            ordered_columns.append(ma_col)\n",
    "\n",
    "# Reorder df_flows\n",
    "df_ax_ma = df_ax_ma[ordered_columns]\n",
    "\n",
    "print(f\"Reordered df_flows columns ({len(df_ax_ma.columns)} total):\")\n",
    "print(\"First 20 columns:\", df_ax_ma.columns[:20].tolist())\n",
    "print(\"...\")\n",
    "print(\"Last 10 columns:\", df_ax_ma.columns[-10:].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d870312",
   "metadata": {},
   "source": [
    "# Button Text Length Analysis\n",
    "Analyze the total length of checkbox button texts and their relationship to the number of rows used in the InteractivePlotWindow frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ad252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import and run the button text analysis\n",
    "# from calculate_button_text_metrics import analyze_button_layout\n",
    "\n",
    "# # Analyze the dataframes that will be used in the interactive chart\n",
    "# # df_flows = left axis (blue frame)\n",
    "# # df_flowdiff = right axis (orange frame)\n",
    "# results = analyze_button_layout(df_ax_ma, df_flowdiff)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"=\" * 80)\n",
    "# print(\"SUMMARY FOR InteractivePlotWindow\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"✓ InteractivePlotWindow now uses OPTIMIZED LAYOUT!\")\n",
    "# print()\n",
    "# print(f\"Row optimization:\")\n",
    "# print(f\"  Left frame will use: {results['optimal_rows_left']} rows\")\n",
    "# print(f\"  Right frame will use: {results['optimal_rows_right']} rows\")\n",
    "# print()\n",
    "# print(f\"Width optimization:\")\n",
    "# print(f\"  Left frame: {results['left']['num_cols']} columns → {results['left']['num_cols']}x stretch\")\n",
    "# print(f\"  Right frame: {results['right']['num_cols']} columns → {results['right']['num_cols']}x stretch\")\n",
    "# print(f\"  Horizontal space allocated proportionally to minimize blank space!\")\n",
    "# print()\n",
    "# print(\"The layout algorithm:\")\n",
    "# print(\"  - Analyzes total character length of button texts\")\n",
    "# print(\"  - Calculates optimal rows (targets ~7 buttons per row)\")\n",
    "# print(\"  - Constrains to min 2 rows, max 4 rows\")\n",
    "# print(\"  - Fills VERTICALLY: row 0, row 1, ..., then next column\")\n",
    "# print(\"  - Allocates horizontal width based on column count ratio\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7028302",
   "metadata": {},
   "source": [
    "# Add elevation levels to df_ax_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d88ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ax_ma = pd.merge(df_ax_ma,df_PlusNivåer, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc93fa8",
   "metadata": {},
   "source": [
    "# Interactive Flow balance chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flows_ma = df_ax_ma.filter(like='[m3/h]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "runchart = 0\n",
    "if runchart == 1:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance() or QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_flows = len(df_flows_ma.columns) + len(df_flowdiff.columns)\n",
    "            settings_key = f'Flöden och Flödesdiff ({n_series_flows} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "            \n",
    "            mainWin = IPW(df_axL = df_flows_ma,\n",
    "                                df_axL_Title = 'Flöde & Bräddflöde [m3/h]', \n",
    "                                df_axR = df_flowdiff, \n",
    "                                df_axR_Title = 'Flödesdiff [%]',\n",
    "                                WindowTitle='Pajala ARV Flöde',\n",
    "                                settings_file=f'InteractivePlotWindow.json::{settings_key}'\n",
    "                            )\n",
    "            mainWin.show()\n",
    "            # Keep references to avoid garbage collection in notebook kernels.\n",
    "            \n",
    "            # Store on the app and module globals so the objects persist after this function returns.\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "            app = _make_and_show()\n",
    "            # Keep references in the IPython user namespace if available so users can interact with them\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    # Fall back to module globals (already set by _make_and_show)\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: start the blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c16d6",
   "metadata": {},
   "source": [
    "# Sludge Treatment Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780541e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Delete variables if they exist (more robust method)\n",
    "# Delete variables if they exist (compact version)\n",
    "for var_name in ['df_sludge_ma', 'df_sludgeflow_ma', 'df_sludgelevel_ma']:\n",
    "    try:\n",
    "        exec(f'del {var_name}')\n",
    "    except NameError:\n",
    "        pass\n",
    "df_sludge_ma = df_ax_ma.filter(regex='Slam|Returslam|AvvatPoly|Slam|Puck|Exten|Rejekt|FlytSlam|[m]').copy()\n",
    "df_sludge_ma['Inflöde Extenslam FT80101 [m3/h]'] = df_AllaFlöden[['Inflöde Extenslam FT80101 [m3/h]']].copy()\n",
    "df_sludge_ma['Inflöde Extenslam FT80101 uncalib [m3/h]'] = df_AllaFlöden[['Inflöde Extenslam FT80101 uncalib [m3/h]']].copy()\n",
    "df_sludge_ma['Inflöde Extenslam FT80101 [m3]'] = (df_sludge_ma['Inflöde Extenslam FT80101 [m3/h]']/60).cumsum()\n",
    "df_sludge_ma['Inflöde Extenslam FT80101 uncalib [m3]'] = (df_sludge_ma['Inflöde Extenslam FT80101 uncalib [m3/h]']/60).cumsum()\n",
    "df_sludge_ma['FlytSlam LT21101 [m*m2]'] = df_sludge_ma['FlytSlam LT21101 [m]'] * 1.25*2\n",
    "df_sludge_ma['SFÖverSlam LT71101 [m]'] = df_Skivfilter['SFÖverSlam LT71101 [m]']\n",
    "df_sludge_ma['SFÖverSlam LT71101 [m*m2]'] = df_Skivfilter['SFÖverSlam LT71101 [m]'] * 1.5*2.2\n",
    "df_sludge_ma['Slamlager 1 LT81101 [m*m2]'] = df_sludge_ma['Slamlager 1 LT81101 [m]'] * 5*7.5\n",
    "df_sludge_ma['Slamlager 2 LT82101 [m*m2]'] = df_sludge_ma['Slamlager 2 LT82101 [m]'] * 3*3.5\n",
    "df_sludge_ma['FörsedUtlopp LT20103 [m]'] = df_Försedimentering['FörsedUtlopp LT20103 [m]']\n",
    "df_sludge_ma['Rejekt LT84101 [m*m2]'] = df_sludge_ma['Rejekt LT84101 [m]'] * 1.2**2/4*math.pi\n",
    "df_sludge_ma['SFIn LT70201 [m]'] = df_Skivfilter['SFIn LT70201 [m]']\n",
    "df_sludge_ma['SFUt LT70102 [m]'] = df_Skivfilter['SFUt LT70102 [m]']\n",
    "df_sludge_ma['SFIn LT70201 [m]'] = df_Skivfilter['SFIn LT70201 [m]']\n",
    "df_sludge_ma['SFUt LT70202 [m]'] = df_Skivfilter['SFUt LT70202 [m]']\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[m3/h\\]_MA').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'uncalibrated').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[l/h\\]_MA').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[1/0\\]').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[Hz\\]').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[m\\]_MA').columns.tolist())\n",
    "df_sludge_ma = df_sludge_ma.drop(columns=df_sludge_ma.filter(regex=r'\\[ppm]').columns.tolist())\n",
    "# df_sludgeflow_ma = df_sludge_ma.filter(regex=r'\\[m3/h\\]')\n",
    "# df_sludgelevel_ma = df_sludge_ma.filter(regex=r'\\[m\\]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Rejektvatten tank nivå analys -  Försök till att separera inflöde och utflöde baserat på nivåändringar (men detta stämmer inte helt när cyklerna är närma uppdateringsintervallet 1 minut)\n",
    "# import pandas as pd\n",
    "# for var_name in ['df_rejekt']:\n",
    "#     try:\n",
    "#         exec(f'del {var_name}')\n",
    "#     except NameError:\n",
    "#         pass\n",
    "# # Calculate the difference between consecutive level readings\n",
    "# df_rejekt = df_sludge_ma[['Rejekt LT84101 [m*m2]']].copy()\n",
    "# df_rejekt['Volym_Delta'] = df_rejekt['Rejekt LT84101 [m*m2]'].diff()\n",
    "\n",
    "# # Classify periods\n",
    "# df_rejekt['Trend'] = pd.NA\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] < 0, 'Trend'] = 'Decreasing'   # Likely pumping\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] > 0, 'Trend'] = 'Increasing'   # Likely filling\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] == 0, 'Trend'] = 'Stable'\n",
    "\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] > 0, 'Inflöde Rejekttank [m*m2]'] = df_rejekt['Volym_Delta']\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] < 0, 'Utflöde Rejekttank [m*m2]'] = -df_rejekt['Volym_Delta']\n",
    "\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] > 0, 'Rejekttank [m3]'] = df_rejekt['Volym_Delta']\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] < 0, 'Rejekttank [m3]'] = -df_rejekt['Volym_Delta']\n",
    "# df_rejekt.loc[df_rejekt['Volym_Delta'] == 0, 'Rejekttank [m3]'] = 0\n",
    "\n",
    "# # Optional: Show only periods with change\n",
    "# df_changes = df_rejekt[df_rejekt['Trend'] != 'Stable']\n",
    "\n",
    "# df_rejekt['Inflöde Rejekttank [m3]'] = df_rejekt['Inflöde Rejekttank [m*m2]'].cumsum()\n",
    "# df_rejekt['Utflöde Rejekttank [m3]'] = df_rejekt['Utflöde Rejekttank [m*m2]'].cumsum()\n",
    "# df_rejekt['Inflöde Rejekttank [m3/h]'] = df_rejekt['Inflöde Rejekttank [m*m2]'] * 60\n",
    "# # df_rejekt.loc[df_rejekt['Inflöde Rejekttank [m*m2]'] != np.nan, 'Inflöde Rejekttank [m3/h]'] = df_rejekt['Volym_Delta'] * 60\n",
    "# # df_rejekt.loc[df_rejekt['Utflöde Rejekttank [m*m2]'] != np.nan, 'Utflöde Rejekttank [m3/h]'] = df_rejekt['Volym_Delta'] * 60\n",
    "\n",
    "# df_sludge_ma = pd.merge(df_sludge_ma, df_rejekt[['Inflöde Rejekttank [m3/h]','Inflöde Rejekttank [m3]', 'Utflöde Rejekttank [m3]']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# print(df_changes.head(20))\n",
    "# print(df_rejekt.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between consecutive level readings\n",
    "for var_name in ['df_slamlager_2']:\n",
    "    try:\n",
    "        exec(f'del {var_name}')\n",
    "    except NameError:\n",
    "        pass\n",
    "df_slamlager_2 = df_sludge_ma[['Slamlager 2 LT82101 [m*m2]']].copy()\n",
    "df_slamlager_2['Volym_Delta [m3/min]'] = df_slamlager_2['Slamlager 2 LT82101 [m*m2]'].diff()\n",
    "df_slamlager_2['Inflöde Slamlager 2 [m3/h]'] = df_slamlager_2['Volym_Delta [m3/min]']*60 - df_sludge_ma['SlamAvvattn FT82101 [m3/h]']\n",
    "df_slamlager_2['Inflöde Rejekttank Puck [m3/h]'] = df_sludge_ma['PuckS FT81101 [m3/h]'].copy() - df_slamlager_2['Inflöde Slamlager 2 [m3/h]'] \n",
    "\n",
    "# Calculate 10-minute moving average (centered on each point)\n",
    "df_slamlager_2['Inflöde Slamlager 2 [m3/h]_MA_10min'] = (\n",
    "    df_slamlager_2['Inflöde Slamlager 2 [m3/h]']\n",
    "    .rolling('10min', min_periods=1, center=True).mean()\n",
    ")\n",
    "\n",
    "# Calculate 10-minute moving average (centered on each point)\n",
    "df_slamlager_2['Inflöde Rejekttank Puck [m3/h]_MA_10min'] = (\n",
    "    df_slamlager_2['Inflöde Rejekttank Puck [m3/h]']\n",
    "    .rolling('10min', min_periods=1, center=True).mean()\n",
    ")\n",
    "df_sludge_ma = pd.merge(df_sludge_ma, df_slamlager_2[['Inflöde Slamlager 2 [m3/h]','Inflöde Slamlager 2 [m3/h]_MA_10min','Inflöde Rejekttank Puck [m3/h]','Inflöde Rejekttank Puck [m3/h]_MA_10min']], left_index=True, right_index=True, how='left')\n",
    "# df_slamlager_2.loc[df_slamlager_2['Volym_Delta [m3/min]'] > 0, 'Inflöde Rejekttank [m3/h]'] = df_slamlager_2['Volym_Delta [m3/min]']\n",
    "# df_slamlager_2.loc[df_slamlager_2['Volym_Delta [m3/min]'] < 0, 'Utflöde Rejekttank [m3/h]'] = -df_slamlager_2['Volym_Delta [m3/min]']\n",
    "# df_slamlager_2.loc[df_slamlager_2['Volym_Delta [m3/min]'] == 0, 'Utflöde Rejekttank [m3/h]'] = 0\n",
    "\n",
    "# Optional: Show only periods with change\n",
    "# df_changes = df_slamlager_2[df_slamlager_2['Trend'] != 'Stable']\n",
    "\n",
    "# df_slamlager_2['Inflöde Rejekttank [m3]'] = df_slamlager_2['Inflöde Rejekttank [m*m2]'].cumsum()\n",
    "# df_slamlager_2['Utflöde Rejekttank [m3]'] = df_slamlager_2['Utflöde Rejekttank [m*m2]'].cumsum()\n",
    "# df_slamlager_2['Inflöde Rejekttank [m3/h]'] = df_slamlager_2['Inflöde Rejekttank [m*m2]'] * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf00779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram över Slamflöden och Slamnivåer\n",
    "from InteractivePlotWindowMultiAxis import InteractivePlotWindowMultiAxis as IPWMultiAxis\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "runchart = 1\n",
    "if runchart == 1:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance()\n",
    "            if app is None:\n",
    "                app = QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_flows = len(df_sludge_ma.columns)\n",
    "            settings_key = f'Pajala ARV Slamflöden och Slamnivåer ({n_series_flows} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "\n",
    "            mainWin = IPWMultiAxis(df_sludge_ma, WindowTitle=\"Pajala ARV Slamflöden och Slamnivåer\")\n",
    "            mainWin.show()\n",
    "            \n",
    "            # Keep references to avoid garbage collection\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook)\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # Enable GUI event loop integration\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            \n",
    "            # Create and show window (no app.exec())\n",
    "            app = _make_and_show()\n",
    "            \n",
    "            # Keep references in IPython user namespace\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1515815",
   "metadata": {},
   "source": [
    "# Disc Filter Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Delete variables if they exist (compact version)\n",
    "for var_name in ['df_discfilter_ma']:\n",
    "    try:\n",
    "        exec(f'del {var_name}')\n",
    "    except NameError:\n",
    "        pass\n",
    "df_discfilter_ma = df_ax_ma.filter(regex=r'flöde|\\[\\+m\\]').copy()\n",
    "# df_discfilter_ma['Inflöde Extenslam FT80101 [m3/h]'] = df_AllaFlöden[['Inflöde Extenslam FT80101 [m3/h]']]\n",
    "# df_discfilter_ma['Inflöde Extenslam FT80101 [m3]'] = (df_discfilter_ma['Inflöde Extenslam FT80101 [m3/h]']/60).cumsum()\n",
    "# df_discfilter_ma['SFÖverSlam LT71101 [m]'] = df_Skivfilter['SFÖverSlam LT71101 [m]']\n",
    "df_discfilter_ma['SFÖverSlam LT71101 [m*m2]'] = df_Skivfilter['SFÖverSlam LT71101 [m]'] * 1.5*2.2\n",
    "# df_discfilter_ma['FörsedUtlopp LT20103 [m]'] = df_Försedimentering['FörsedUtlopp LT20103 [m]']\n",
    "# df_discfilter_ma['Rejekt LT84101 [m*m2]'] = df_discfilter_ma['Rejekt LT84101 [m]'] * 1.2**2/4*math.pi\n",
    "\n",
    "# df_discfilter_ma['SFIN LT70101 [m]'] = df_Skivfilter['SFIn LT70101 [m]']\n",
    "# df_discfilter_ma['SFUt LT70102 [m]'] = df_Skivfilter['SFUt LT70102 [m]']\n",
    "# df_discfilter_ma['SFIn LT70201 [m]'] = df_Skivfilter['SFIn LT70201 [m]']\n",
    "# df_discfilter_ma['SFUt LT70202 [m]'] = df_Skivfilter['SFUt LT70202 [m]']\n",
    "# df_discfilter_ma['MBBR LT31101 [m]'] = df_MBBR['MBBR LT31101 [m]']\n",
    "# df_discfilter_ma['MBBR LT31201 [m]'] = df_MBBR['MBBR LT31201 [m]']\n",
    "\n",
    "# df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[m3/h\\]_MA').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'uncalibrated').columns.tolist())\n",
    "# df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[l/h\\]_MA').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[1/0\\]').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[Hz\\]').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[m\\]_MA').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[m\\]').columns.tolist())\n",
    "df_discfilter_ma = df_discfilter_ma.drop(columns=df_discfilter_ma.filter(regex=r'\\[\\+m\\]_MA').columns.tolist())\n",
    "# df_sludgeflow_ma = df_sludge_ma.filter(regex=r'\\[m3/h\\]')\n",
    "# df_sludgelevel_ma = df_sludge_ma.filter(regex=r'\\[m\\]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram över Skivfilter flöden och nivåer\n",
    "from InteractivePlotWindowMultiAxis import InteractivePlotWindowMultiAxis as IPWMultiAxis\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "runchart = 0\n",
    "if runchart == 1:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance()\n",
    "            if app is None:\n",
    "                app = QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_flows = len(df_discfilter_ma.columns)\n",
    "            settings_key = f'Pajala ARV Skivfilter ({n_series_flows} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "\n",
    "            mainWin = IPWMultiAxis(df_discfilter_ma, WindowTitle=\"Pajala ARV Skivfilter\")\n",
    "            mainWin.show()\n",
    "            \n",
    "            # Keep references to avoid garbage collection\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook)\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # Enable GUI event loop integration\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            \n",
    "            # Create and show window (no app.exec())\n",
    "            app = _make_and_show()\n",
    "            \n",
    "            # Keep references in IPython user namespace\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8652b62",
   "metadata": {},
   "source": [
    "# MBBR Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables if they exist (compact version)\n",
    "for var_name in ['df_MBBR_ma', 'df_MBBRflow_ma', 'df_MBBRlevel_ma']:\n",
    "    try:\n",
    "        exec(f'del {var_name}')\n",
    "    except NameError:\n",
    "        pass    \n",
    "df_MBBR_ma = df_ax_ma.filter(regex='flöde|MBBR', axis=1)\n",
    "# df_MBBRflow_ma = df_MBBR_ma.filter(regex=r'\\[m3/h\\]')\n",
    "# df_MBBRlevel_ma = df_MBBR_ma.filter(regex=r'\\[m\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a81545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractivePlotWindowMultiAxis import InteractivePlotWindowMultiAxis as IPWMultiAxis\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "import sys\n",
    "\n",
    "runchart = 0\n",
    "if runchart == 1:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance() or QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_flows = len(df_MBBR_ma.columns) # + len(df_sludgelevel_ma.columns)\n",
    "            settings_key = f'Pajala ARV MBBR ({n_series_flows} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "\n",
    "            app = QApplication(sys.argv)\n",
    "            mainWin = IPWMultiAxis(df_MBBR_ma, WindowTitle=\"Multi-Unit Monitoring Test\")\n",
    "            mainWin.show()\n",
    "            mainWin.show()\n",
    "            # Keep references to avoid garbage collection in notebook kernels.\n",
    "            \n",
    "            # Store on the app and module globals so the objects persist after this function returns.\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "            app = _make_and_show()\n",
    "            # Keep references in the IPython user namespace if available so users can interact with them\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    # Fall back to module globals (already set by _make_and_show)\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: start the blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294b2a1",
   "metadata": {},
   "source": [
    "# Interactive Velocity Chart (FT10101)\n",
    "Interactive plot showing flow velocity in m/s with flow differences on the right axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "# from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "# Align the velocity (with MAs) and flowdiff DataFrames to the same index\n",
    "union_idx_velocity = df_velocity_with_ma.index.union(df_flowdiff.index)\n",
    "df_velocity_aligned = df_velocity_with_ma.reindex(union_idx_velocity)\n",
    "df_flowdiff_aligned = df_flowdiff.reindex(union_idx_velocity)\n",
    "\n",
    "print(f\"Aligned velocity DataFrame shape: {df_velocity_aligned.shape}\")\n",
    "print(f\"Aligned flowdiff DataFrame shape: {df_flowdiff_aligned.shape}\")\n",
    "\n",
    "runchart = 0\n",
    "if runchart == 1:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show_velocity():\n",
    "            app = QApplication.instance() or QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_velocity = len(df_velocity_aligned.columns) + len(df_flowdiff_aligned.columns)\n",
    "            settings_key_velocity = f'Hastighet FT10101 och Flödesdiff ({n_series_velocity} series)'\n",
    "            print(f\"Chart settings key: {settings_key_velocity}\")\n",
    "            \n",
    "            mainWin_velocity = IPW(\n",
    "                df_axL = df_velocity_aligned,\n",
    "                df_axL_Title = 'Hastighet FT10101 [m/s]', \n",
    "                df_axR = df_flowdiff_aligned, \n",
    "                df_axR_Title = 'Flödesdiff [%]',\n",
    "                WindowTitle='Pajala ARV - Hastighet FT10101 och Flödesdiff',\n",
    "                settings_file=f'InteractivePlotWindow.json::{settings_key_velocity}'\n",
    "            )\n",
    "            mainWin_velocity.show()\n",
    "            # Keep references to avoid garbage collection in notebook kernels.\n",
    "            \n",
    "            # Store on the app and module globals so the objects persist after this function returns.\n",
    "            try:\n",
    "                app._pajala_velocity_mainWin = mainWin_velocity\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_velocity_mainWin'] = mainWin_velocity\n",
    "            globals()['_pajala_velocity_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "            app = _make_and_show_velocity()\n",
    "            # Keep references in the IPython user namespace if available so users can interact with them\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_velocity_app'] = app\n",
    "                    ip.user_ns['_pajala_velocity_mainWin'] = globals().get('_pajala_velocity_mainWin')\n",
    "                except Exception:\n",
    "                    # Fall back to module globals (already set by _make_and_show_velocity)\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: start the blocking event loop\n",
    "            app = _make_and_show_velocity()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed34f0",
   "metadata": {},
   "source": [
    "# Flow Difference vs Velocity Correlation Chart\n",
    "Cross-plot showing the relationship between flow velocity and flow differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4 separate XY scatter plots: Velocity (X) vs Flow Difference (Y)\n",
    "# 1. Raw data (no MA)\n",
    "# 2. MA_1h vs MA_1h\n",
    "# 3. MA_24h vs MA_24h\n",
    "# 4. MA_7d vs MA_7d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the pairs to plot: (velocity_column, flowdiff_column, title_suffix)\n",
    "plot_pairs = [\n",
    "    ('Inflöde FT10101 [m/s]', 'Diff_MA_1h', 'Raw Data'),\n",
    "    ('Inflöde FT10101 [m/s]_MA_1h', 'Diff_MA_1h', 'MA 1h'),\n",
    "    ('Inflöde FT10101 [m/s]_MA_24h', 'Diff_MA_24h', 'MA 24h'),\n",
    "    ('Inflöde FT10101 [m/s]_MA_7d', 'Diff_MA_7d', 'MA 7d')\n",
    "]\n",
    "\n",
    "# Create 2x2 subplot layout\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (velocity_col, flowdiff_col, title_suffix) in enumerate(plot_pairs):\n",
    "    ax = axes[idx]\n",
    "    dprint(flowdiff_col)\n",
    "    # Get non-NaN pairs only\n",
    "    mask = df_velocity_aligned[velocity_col].notna() & df_flowdiff_aligned[flowdiff_col].notna()\n",
    "    x_data = df_velocity_aligned.loc[mask, velocity_col]\n",
    "    y_data = df_flowdiff_aligned.loc[mask, flowdiff_col]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    ax.scatter(x_data, y_data, alpha=0.3, s=1, color='blue')\n",
    "    \n",
    "    ax.set_xlabel('Hastighet FT10101 [m/s]', fontsize=11)\n",
    "    ax.set_ylabel('Flödesdiff [%]', fontsize=11)\n",
    "    ax.set_title(f'Flödesdiff vs Hastighet - {title_suffix}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add data point count\n",
    "    ax.text(0.02, 0.98, f'n = {mask.sum():,}', \n",
    "            transform=ax.transAxes, \n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Created 4 scatter plots:\")\n",
    "print(\"  1. Raw velocity vs Diff_MA_1h\")\n",
    "print(\"  2. Velocity MA_1h vs Diff_MA_1h\")\n",
    "print(\"  3. Velocity MA_24h vs Diff_MA_24h\")\n",
    "print(\"  4. Velocity MA_7d vs Diff_MA_7d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d77ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross-Correlation Analysis\n",
    "# from scipy.signal import correlate\n",
    "# import numpy as np\n",
    "\n",
    "# # Calculate cross-correlation\n",
    "# correlation = correlate(df_ax['Inflöde FT10101'].fillna(0), \n",
    "#                         df_ax['Utflöde FT72101'].fillna(0), \n",
    "#                         mode='full')\n",
    "\n",
    "# # Find the lag that gives maximum correlation\n",
    "# lags = np.arange(-len(df_ax) + 1, len(df_ax))\n",
    "# optimal_lag_idx = np.argmax(correlation)\n",
    "# optimal_lag = lags[optimal_lag_idx]\n",
    "\n",
    "# print(f\"Optimal lag: {optimal_lag} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76584477",
   "metadata": {},
   "source": [
    "# Cumulative Flow Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative volumes\n",
    "df_cumsumflows = pd.DataFrame(index=df_ax.index)\n",
    "df_cumsumflows['Inflöde FT10101_[m3]]'] = df_ax['Inflöde FT10101 [m3/h]'].cumsum()\n",
    "df_cumsumflows['Inflöde Extenslam FT80101_[m3]'] = df_ax['Inflöde Extenslam FT80101 [m3/h]'].cumsum()\n",
    "df_cumsumflows['Utflöde FT72101_[m3]'] = df_ax['Utflöde FT72101 [m3/h]'].cumsum()\n",
    "df_cumsumflows['MBBRflöde FT30101_[m3]'] = df_ax['MBBRflöde FT30101 [m3/h]'].cumsum()\n",
    "df_cumsumflows['Utflöde Bräddning LT23101_[m3]'] = df_ax['Utflöde Bräddning LT23101 [m3/h]'].cumsum()\n",
    "\n",
    "df_cumsumflows['Inflow_[m3]'] = (df_ax['Inflöde FT10101 [m3/h]'] + \n",
    "                          df_ax['Inflöde Extenslam FT80101 [m3/h]']).cumsum()\n",
    "df_cumsumflows['Outflow_[m3]'] = (df_ax['Utflöde FT72101 [m3/h]'] + \n",
    "                           df_ax['Utflöde Bräddning LT23101 [m3/h]']).cumsum()\n",
    "\n",
    "# Storage change = Cumulative inflow - Cumulative outflow\n",
    "df_cumsumflows['Storage_Change_[m3]'] = df_cumsumflows['Inflow_[m3]'] - df_cumsumflows['Outflow_[m3]']\n",
    "df_cumsumflows['Storage_Change_Percent'] = df_cumsumflows['Storage_Change_[m3]'] / df_cumsumflows['Inflow_[m3]'] * 100.0\n",
    "# df_flow_cumsum = pd.merge(df_flows,df_cumsumflows, left_index=True, right_index=True, how='outer'                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f60ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "if False:\n",
    "    if __name__ == \"__main__\":\n",
    "        import sys as _sys\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        def _make_and_show():\n",
    "            app = QApplication.instance() or QApplication(_sys.argv)\n",
    "            \n",
    "            # Calculate total number of series for settings file key\n",
    "            n_series_flows = len(df_ax_ma.columns) + len(df_flowdiff.columns)\n",
    "            settings_key = f'Kumsumflöden in och ut ({n_series_flows} series)'\n",
    "            print(f\"Chart settings key: {settings_key}\")\n",
    "            \n",
    "            mainWin = IPW(df_axL = df_ax_ma,\n",
    "                                df_axL_Title = 'Flöde [m3/h]', \n",
    "                                df_axR = df_cumsumflows, \n",
    "                                df_axR_Title = 'Kumsumflöde [m3]',\n",
    "                                WindowTitle='Pajala ARV Flöden och kumsumflöden',\n",
    "                                settings_file=f'InteractivePlotWindow.json::{settings_key}'\n",
    "                            )\n",
    "            mainWin.show()\n",
    "            # Keep references to avoid garbage collection in notebook kernels.\n",
    "            \n",
    "            # Store on the app and module globals so the objects persist after this function returns.\n",
    "            try:\n",
    "                app._pajala_mainWin = mainWin\n",
    "            except Exception:\n",
    "                pass\n",
    "            globals()['_pajala_mainWin'] = mainWin\n",
    "            globals()['_pajala_app'] = app\n",
    "            return app\n",
    "\n",
    "        # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "        if 'ipykernel' in _sys.modules:\n",
    "            try:\n",
    "                ip = get_ipython()\n",
    "                if ip is not None:\n",
    "                    # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                    ip.run_line_magic('gui', 'qt')\n",
    "            except Exception:\n",
    "                ip = None\n",
    "            # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "            app = _make_and_show()\n",
    "            # Keep references in the IPython user namespace if available so users can interact with them\n",
    "            if ip is not None:\n",
    "                try:\n",
    "                    ip.user_ns['_pajala_app'] = app\n",
    "                    ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "                except Exception:\n",
    "                    # Fall back to module globals (already set by _make_and_show)\n",
    "                    pass\n",
    "        else:\n",
    "            # Running as a script: start the blocking event loop\n",
    "            app = _make_and_show()\n",
    "            _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
