{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d350c3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Pajala ARV Flöden\n",
    "Data leverarad av Kristofer Grammer <kristofer.gramner@gefasystem.se>. Data bearbetat och diagram skapade av Christian Nilsson med hjälp av Github Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fb41e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .\\.venv\\Scripts\\Activate.ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c7cf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "def dprint(x): # https://stackoverflow.com/questions/32000934/print-a-variables-name-and-value/57225950#57225950\n",
    "    frame = inspect.currentframe().f_back\n",
    "    s = inspect.getframeinfo(frame).code_context[0]\n",
    "    r = re.search(r\"\\((.*)\\)\", s).group(1)\n",
    "    print(\"{} = {}\".format(r,x))\n",
    "\n",
    "def _pk1_path_for_file(file_path):\n",
    "    \"\"\"Return Path for .pk1 cache file stored next to the input file with same base name and extension '.pk1'.\"\"\"\n",
    "    p = Path(file_path) if not isinstance(file_path, Path) else file_path\n",
    "    return p.with_suffix('.pk1')\n",
    "\n",
    "def load_or_cache_excel(xlsx_path, read_kwargs=None, force_refresh=False):\n",
    "    \"\"\"Load DataFrame from a .pk1 cache next to the xlsx if present; otherwise read the xlsx and save the .pk1.\n",
    "    Returns the DataFrame.\n",
    "    read_kwargs: dict forwarded to pd.read_excel.\n",
    "    force_refresh: if True, re-read the Excel and overwrite cache.\"\"\"\n",
    "    read_kwargs = read_kwargs or {}\n",
    "    pk1 = _pk1_path_for_file(xlsx_path)\n",
    "    if pk1.exists() and not force_refresh:\n",
    "        try:\n",
    "            df = pd.read_pickle(pk1)\n",
    "            print(f'Loaded cache {pk1}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to load {pk1} (will re-read Excel): {e}')\n",
    "    # read Excel and attempt to save cache\n",
    "    df = pd.read_excel(xlsx_path, **read_kwargs)\n",
    "    try:\n",
    "        df.to_pickle(pk1)\n",
    "        print(f'Saved cache {pk1}')\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not save cache {pk1}: {e}')\n",
    "    return df\n",
    "\n",
    "def load_or_cache_csv(csv_path, read_kwargs=None, force_refresh=False):\n",
    "    \"\"\"Load DataFrame from a .pk1 cache next to the csv if present; otherwise read the csv and save the .pk1.\n",
    "    Returns the DataFrame.\n",
    "    read_kwargs: dict forwarded to pd.read_csv.\n",
    "    force_refresh: if True, re-read the CSV and overwrite cache.\"\"\"\n",
    "    read_kwargs = read_kwargs or {}\n",
    "    # Set default read parameters for our specific CSV format\n",
    "    default_params = {\n",
    "        'sep': ';',  # semicolon separated\n",
    "        'decimal': ',',  # comma as decimal separator\n",
    "        'parse_dates': ['TimeDate'],  # parse TimeDate column as datetime\n",
    "    }\n",
    "    # Update with any user-provided parameters\n",
    "    read_kwargs = {**default_params, **read_kwargs}\n",
    "    \n",
    "    pk1 = _pk1_path_for_file(csv_path)\n",
    "    if pk1.exists() and not force_refresh:\n",
    "        try:\n",
    "            df = pd.read_pickle(pk1)\n",
    "            print(f'Loaded cache {pk1}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to load {pk1} (will re-read CSV): {e}')\n",
    "    # read CSV and attempt to save cache\n",
    "    df = pd.read_csv(csv_path, **read_kwargs)\n",
    "    try:\n",
    "        df.to_pickle(pk1)\n",
    "        print(f'Saved cache {pk1}')\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not save cache {pk1}: {e}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40bfd7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT10101.pk1\n",
      "Loaded cache c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT72101.pk1\n",
      "Loaded cache c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT30101.pk1\n",
      "Loaded cache c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT80101.pk1\n",
      "Loaded cache c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\LT23101.pk1\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files (with pk1 cache next to each csv). Uses load_or_cache_csv from previous cell.\n",
    "csv_file_path_FT10101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT10101.csv'\n",
    "csv_file_path_FT30101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT30101.csv'\n",
    "csv_file_path_FT72101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT72101.csv'\n",
    "csv_file_path_FT80101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\FT80101.csv'\n",
    "csv_file_path_LT23101 = r'c:\\Users\\chrini\\OneDrive - Norconsult Group\\Projekt\\1097224_Pajala ARV\\4 Underlag\\Mejl\\20251104_1609_Re_ Malmberg Water i Yngsjö - ARV Pajala, åtgärd diverse styrpunkter_Kristofer Gramner\\20251104\\LT23101.csv'\n",
    "\n",
    "# Load using the helper which places the .pk1 next to the csv with same base name\n",
    "df_Inflöde_FT10101 = load_or_cache_csv(csv_file_path_FT10101) #Inflöde\n",
    "df_Utflöde_FT72101 = load_or_cache_csv(csv_file_path_FT72101) #Utflöde\n",
    "df_MBBRflöde_FT30101 = load_or_cache_csv(csv_file_path_FT30101) #MBBR-flöde\n",
    "df_Inflöde_Extenslam_FT80101 = load_or_cache_csv(csv_file_path_FT80101) #Inflöde Extenslam\n",
    "df_Nivå_Bräddning_LT23101 = load_or_cache_csv(csv_file_path_LT23101) #Utflöde Bräddning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "856a7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_Utflöde_FT72101.head() =                      Utflöde FT-72101\n",
      "DateTime                             \n",
      "2024-11-01 03:01:00         71.990110\n",
      "2024-11-01 03:02:00         37.916668\n",
      "2024-11-01 03:03:00         17.846454\n",
      "2024-11-01 03:04:00         11.633287\n",
      "2024-11-01 03:05:00         45.686008\n",
      "df_Inflöde_Extenslam_FT80101.head() =                      Inflöde Extenslam FT80101\n",
      "DateTime                                      \n",
      "2024-11-01 03:01:00                  -0.486475\n",
      "2024-11-01 03:02:00                  -0.486475\n",
      "2024-11-01 03:03:00                  -0.486475\n",
      "2024-11-01 03:04:00                  -0.486475\n",
      "2024-11-01 03:05:00                  -0.486475\n",
      "df_Nivå_Bräddning_LT23101.head() =                      Nivå Bräddning LT23101\n",
      "DateTime                                   \n",
      "2024-11-01 03:01:00                0.148113\n",
      "2024-11-01 03:02:00                0.148113\n",
      "2024-11-01 03:03:00                0.148113\n",
      "2024-11-01 03:04:00                0.148113\n",
      "2024-11-01 03:05:00                0.148113\n",
      "\n",
      "Merged DataFrame:\n",
      "df_ax.head() =                      Inflöde FT-10101  Utflöde FT-72101  \\\n",
      "DateTime                                                  \n",
      "2024-11-01 03:01:00         48.163444         71.990110   \n",
      "2024-11-01 03:02:00         53.358573         37.916668   \n",
      "2024-11-01 03:03:00         48.372349         17.846454   \n",
      "2024-11-01 03:04:00         44.189613         11.633287   \n",
      "2024-11-01 03:05:00         24.777310         45.686008   \n",
      "\n",
      "                     Inflöde Extenslam FT80101  \n",
      "DateTime                                        \n",
      "2024-11-01 03:01:00                        0.0  \n",
      "2024-11-01 03:02:00                        0.0  \n",
      "2024-11-01 03:03:00                        0.0  \n",
      "2024-11-01 03:04:00                        0.0  \n",
      "2024-11-01 03:05:00                        0.0  \n",
      "\n",
      "Missing values in merged DataFrame:\n",
      "df_ax.isna().sum() = Inflöde FT-10101             7\n",
      "Utflöde FT-72101             1\n",
      "Inflöde Extenslam FT80101    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dprint(df_1.head())\n",
    "df_Inflöde_FT10101.rename(columns={'TimeDate': 'DateTime', 'Val':'Inflöde FT-10101'}, inplace=True)\n",
    "df_Inflöde_FT10101['DateTime'] = pd.to_datetime(df_Inflöde_FT10101['DateTime'])\n",
    "df_Inflöde_FT10101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "df_Inflöde_FT10101.set_index('DateTime', inplace=True)\n",
    "# dprint(df_1.head())\n",
    "\n",
    "\n",
    "# print(df_2.head())\n",
    "df_Utflöde_FT72101.rename(columns={'TimeDate': 'DateTime', 'Val':'Utflöde FT-72101'}, inplace=True)\n",
    "df_Utflöde_FT72101['DateTime'] = pd.to_datetime(df_Utflöde_FT72101['DateTime'])\n",
    "df_Utflöde_FT72101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "df_Utflöde_FT72101.set_index('DateTime', inplace=True)\n",
    "dprint(df_Utflöde_FT72101.head())\n",
    "\n",
    "df_Inflöde_Extenslam_FT80101.rename(columns={'TimeDate': 'DateTime', 'Val':'Inflöde Extenslam FT80101'}, inplace=True)\n",
    "df_Inflöde_Extenslam_FT80101['DateTime'] = pd.to_datetime(df_Inflöde_Extenslam_FT80101['DateTime'])\n",
    "df_Inflöde_Extenslam_FT80101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "df_Inflöde_Extenslam_FT80101.set_index('DateTime', inplace=True)\n",
    "dprint(df_Inflöde_Extenslam_FT80101.head())\n",
    "df_Inflöde_Extenslam_FT80101_before_zeroflow_calib = df_Inflöde_Extenslam_FT80101\n",
    "df_Inflöde_Extenslam_FT80101['Inflöde Extenslam FT80101'] = df_Inflöde_Extenslam_FT80101['Inflöde Extenslam FT80101'] + 0.486474692821503\n",
    "\n",
    "df_Nivå_Bräddning_LT23101.rename(columns={'TimeDate': 'DateTime', 'Val':'Nivå Bräddning LT23101'}, inplace=True)\n",
    "df_Nivå_Bräddning_LT23101['DateTime'] = pd.to_datetime(df_Nivå_Bräddning_LT23101['DateTime'])\n",
    "df_Nivå_Bräddning_LT23101.drop(columns=['ID', 'TimeLength'], inplace=True)\n",
    "df_Nivå_Bräddning_LT23101.set_index('DateTime', inplace=True)\n",
    "dprint(df_Nivå_Bräddning_LT23101.head())\n",
    "\n",
    "# Merge the DataFrames on the DateTime index, aligning values\n",
    "# Using merge instead of concat to handle any duplicate indices\n",
    "# Merge all DataFrames sequentially\n",
    "df_ax = pd.merge(df_Inflöde_FT10101, df_Utflöde_FT72101, left_index=True, right_index=True, how='outer')\n",
    "df_ax = pd.merge(df_ax, df_Inflöde_Extenslam_FT80101, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Show the result\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "dprint(df_ax.head())\n",
    "\n",
    "# Check for any missing values after merge\n",
    "print(\"\\nMissing values in merged DataFrame:\")\n",
    "dprint(df_ax.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f17e3f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_Inflöde_FT10101: ['Inflöde FT-10101']\n",
      "Columns in df_Utflöde_FT72101: ['Utflöde FT-72101']\n",
      "Columns in df_ax: ['Inflöde FT-10101', 'Utflöde FT-72101', 'Inflöde Extenslam FT80101']\n"
     ]
    }
   ],
   "source": [
    "# Show all column names to verify what needs renaming\n",
    "print(\"Columns in df_Inflöde_FT10101:\", df_Inflöde_FT10101.columns.tolist())\n",
    "print(\"Columns in df_Utflöde_FT72101:\", df_Utflöde_FT72101.columns.tolist())\n",
    "print(\"Columns in df_ax:\", df_ax.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09c70e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DateTime indices with missing FT-10101:\n",
      "['2025-11-04 15:44:00', '2025-11-04 15:45:00', '2025-11-04 15:46:00', '2025-11-04 15:47:00', '2025-11-04 15:48:00', '2025-11-04 15:49:00', '2025-11-04 15:50:00']\n",
      "\n",
      "DateTime indices with missing FT-72101:\n",
      "['2025-11-04 15:50:00']\n",
      "\n",
      "Summary of gaps:\n",
      "Total rows in merged DataFrame: 513884\n",
      "Rows with missing FT-10101: 7\n",
      "Rows with missing FT-72101: 1\n",
      "Rows with data in both columns: 513877\n"
     ]
    }
   ],
   "source": [
    "# Show DateTime indices where there are missing values\n",
    "print(\"\\nDateTime indices with missing FT-10101:\")\n",
    "print(df_ax[df_ax['Inflöde FT-10101'].isna()].index.strftime('%Y-%m-%d %H:%M:%S').tolist())\n",
    "\n",
    "print(\"\\nDateTime indices with missing FT-72101:\")\n",
    "print(df_ax[df_ax['Utflöde FT-72101'].isna()].index.strftime('%Y-%m-%d %H:%M:%S').tolist())\n",
    "\n",
    "# Print summary of gaps\n",
    "print(\"\\nSummary of gaps:\")\n",
    "print(f\"Total rows in merged DataFrame: {len(df_ax)}\")\n",
    "print(f\"Rows with missing FT-10101: {df_ax['Inflöde FT-10101'].isna().sum()}\")\n",
    "print(f\"Rows with missing FT-72101: {df_ax['Utflöde FT-72101'].isna().sum()}\")\n",
    "print(f\"Rows with data in both columns: {len(df_ax) - df_ax.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133f22f",
   "metadata": {},
   "source": [
    "# Check and Remove Duplicate Timestamps\n",
    "Before calculating moving averages, we need to identify and remove any duplicate timestamps from the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a4a0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: df_ax has 7 duplicate indices. Keeping first occurrence.\n",
      "Warning: df_Inflöde_Extenslam_FT80101 has 1 duplicate indices. Keeping first occurrence.\n",
      "Warning: df_Nivå_Bräddning_LT23101 has 1 duplicate indices. Keeping first occurrence.\n",
      "\n",
      "Total duplicate rows collected: 12\n",
      "Unique duplicate timestamps: 1\n",
      "Duplicate timestamps saved to: duplicate_timestamps.csv\n",
      "\n",
      "Cleaned DataFrame sizes:\n",
      "  df_ax: 513877 rows\n",
      "  df_Inflöde_Extenslam_FT80101: 513877 rows\n",
      "  df_Nivå_Bräddning_LT23101: 513892 rows\n"
     ]
    }
   ],
   "source": [
    "# Collect all duplicate timestamps before removing them\n",
    "df_duplicate_timestamps = pd.DataFrame()\n",
    "\n",
    "# Helper function to collect duplicates from a DataFrame\n",
    "def collect_duplicates(df, df_name):\n",
    "    \"\"\"Collect duplicate rows and return them with a source column.\"\"\"\n",
    "    if df.index.duplicated().any():\n",
    "        dup_mask = df.index.duplicated(keep=False)  # Mark ALL duplicates, not just subsequent ones\n",
    "        dup_rows = df[dup_mask].copy()\n",
    "        dup_rows['Source'] = df_name\n",
    "        dup_rows['DuplicateGroup'] = dup_rows.index.astype(str)\n",
    "        return dup_rows\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Check for and collect duplicate indices before removal\n",
    "frames_to_check = {\n",
    "    'df_ax': df_ax,\n",
    "    'df_Inflöde_Extenslam_FT80101': df_Inflöde_Extenslam_FT80101,\n",
    "    'df_Nivå_Bräddning_LT23101': df_Nivå_Bräddning_LT23101\n",
    "}\n",
    "\n",
    "duplicate_collections = []\n",
    "for name, df_frame in frames_to_check.items():\n",
    "    if df_frame.index.duplicated().any():\n",
    "        dup_count = df_frame.index.duplicated().sum()\n",
    "        print(f\"Warning: {name} has {dup_count} duplicate indices. Keeping first occurrence.\")\n",
    "        \n",
    "        # Collect duplicates\n",
    "        dup_df = collect_duplicates(df_frame, name)\n",
    "        if not dup_df.empty:\n",
    "            duplicate_collections.append(dup_df)\n",
    "        \n",
    "        # Remove duplicates from the original frame\n",
    "        if name == 'df_ax':\n",
    "            df_ax = df_ax[~df_ax.index.duplicated(keep='first')]\n",
    "        elif name == 'df_Inflöde_Extenslam_FT80101':\n",
    "            df_Inflöde_Extenslam_FT80101 = df_Inflöde_Extenslam_FT80101[~df_Inflöde_Extenslam_FT80101.index.duplicated(keep='first')]\n",
    "        elif name == 'df_Nivå_Bräddning_LT23101':\n",
    "            df_Nivå_Bräddning_LT23101 = df_Nivå_Bräddning_LT23101[~df_Nivå_Bräddning_LT23101.index.duplicated(keep='first')]\n",
    "\n",
    "# Combine all duplicate collections into one DataFrame\n",
    "if duplicate_collections:\n",
    "    df_duplicate_timestamps = pd.concat(duplicate_collections, axis=0)\n",
    "    df_duplicate_timestamps = df_duplicate_timestamps.sort_values(['DuplicateGroup', 'Source'])\n",
    "    print(f\"\\nTotal duplicate rows collected: {len(df_duplicate_timestamps)}\")\n",
    "    print(f\"Unique duplicate timestamps: {df_duplicate_timestamps.index.nunique()}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = 'duplicate_timestamps.csv'\n",
    "    df_duplicate_timestamps.to_csv(csv_path)\n",
    "    print(f\"Duplicate timestamps saved to: {csv_path}\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates found in any DataFrame.\")\n",
    "\n",
    "print(f\"\\nCleaned DataFrame sizes:\")\n",
    "print(f\"  df_ax: {len(df_ax)} rows\")\n",
    "print(f\"  df_Inflöde_Extenslam_FT80101: {len(df_Inflöde_Extenslam_FT80101)} rows\")\n",
    "print(f\"  df_Nivå_Bräddning_LT23101: {len(df_Nivå_Bräddning_LT23101)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3605581",
   "metadata": {},
   "source": [
    "# Calculate Velocity for FT-10101\n",
    "Convert flow rate (m³/s) to velocity (m/s) using pipe inside diameter of 300 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "caf1a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe inside diameter: 300.0 mm\n",
      "Cross-sectional area: 0.070686 m²\n",
      "\n",
      "Velocity statistics:\n",
      "count    5.138700e+05\n",
      "mean     1.385685e-01\n",
      "std      5.307203e-02\n",
      "min     -6.012520e-08\n",
      "25%      1.078246e-01\n",
      "50%      1.340951e-01\n",
      "75%      1.617729e-01\n",
      "max      7.404189e-01\n",
      "Name: Inflöde FT-10101 [m/s], dtype: float64\n",
      "\n",
      "First few values:\n",
      "                     Inflöde FT-10101 [m/s]\n",
      "DateTime                                   \n",
      "2024-11-01 03:01:00                0.189270\n",
      "2024-11-01 03:02:00                0.209686\n",
      "2024-11-01 03:03:00                0.190091\n",
      "2024-11-01 03:04:00                0.173654\n",
      "2024-11-01 03:05:00                0.097369\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pipe inside diameter in meters\n",
    "diameter_m = 0.300  # 300 mm = 0.3 m\n",
    "\n",
    "# Calculate cross-sectional area: A = π × (d/2)²\n",
    "area_m2 = np.pi * (diameter_m / 2) ** 2\n",
    "\n",
    "print(f\"Pipe inside diameter: {diameter_m * 1000} mm\")\n",
    "print(f\"Cross-sectional area: {area_m2:.6f} m²\")\n",
    "\n",
    "# Calculate velocity: v = Q / A\n",
    "# df_ax['Inflöde FT-10101'] is in m³/s (assuming flow rate units)\n",
    "# Velocity will be in m/s\n",
    "df_Inflöde_FT10101_mps = pd.DataFrame(index=df_ax.index)\n",
    "df_Inflöde_FT10101_mps['Inflöde FT-10101 [m/s]'] = df_ax['Inflöde FT-10101'] / 3.6 / 1000 / area_m2\n",
    "\n",
    "print(f\"\\nVelocity statistics:\")\n",
    "print(df_Inflöde_FT10101_mps['Inflöde FT-10101 [m/s]'].describe())\n",
    "print(f\"\\nFirst few values:\")\n",
    "print(df_Inflöde_FT10101_mps.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b8cf3",
   "metadata": {},
   "source": [
    "# Calculate Moving Averages for Velocity\n",
    "Apply the same time-based moving average windows (1h, 24h, 7d) to the velocity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7319c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Velocity DataFrame with Moving Averages:\n",
      "Shape: (513877, 4)\n",
      "Columns: ['Inflöde FT-10101 [m/s]', 'Inflöde FT-10101 [m/s]_MA_1h', 'Inflöde FT-10101 [m/s]_MA_24h', 'Inflöde FT-10101 [m/s]_MA_7d']\n",
      "\n",
      "First few rows:\n",
      "                     Inflöde FT-10101 [m/s]  Inflöde FT-10101 [m/s]_MA_1h  \\\n",
      "DateTime                                                                    \n",
      "2024-11-01 03:01:00                0.189270                      0.189270   \n",
      "2024-11-01 03:02:00                0.209686                      0.199478   \n",
      "2024-11-01 03:03:00                0.190091                      0.196349   \n",
      "2024-11-01 03:04:00                0.173654                      0.190675   \n",
      "2024-11-01 03:05:00                0.097369                      0.172014   \n",
      "\n",
      "                     Inflöde FT-10101 [m/s]_MA_24h  \\\n",
      "DateTime                                             \n",
      "2024-11-01 03:01:00                       0.189270   \n",
      "2024-11-01 03:02:00                       0.199478   \n",
      "2024-11-01 03:03:00                       0.196349   \n",
      "2024-11-01 03:04:00                       0.190675   \n",
      "2024-11-01 03:05:00                       0.172014   \n",
      "\n",
      "                     Inflöde FT-10101 [m/s]_MA_7d  \n",
      "DateTime                                           \n",
      "2024-11-01 03:01:00                      0.189270  \n",
      "2024-11-01 03:02:00                      0.199478  \n",
      "2024-11-01 03:03:00                      0.196349  \n",
      "2024-11-01 03:04:00                      0.190675  \n",
      "2024-11-01 03:05:00                      0.172014  \n"
     ]
    }
   ],
   "source": [
    "# Calculate moving averages for velocity data\n",
    "# Prepare container for velocity moving averages\n",
    "df_velocity_ma = pd.DataFrame(index=df_Inflöde_FT10101_mps.index)\n",
    "\n",
    "# Define the same time-based windows as used for flows\n",
    "windows_velocity = { '1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "\n",
    "for col in df_Inflöde_FT10101_mps.columns:\n",
    "    for w_label, w_offset in windows_velocity.items():\n",
    "        # Use time-based rolling which is robust to missing/irregular timestamps\n",
    "        ma = df_Inflöde_FT10101_mps[col].rolling(w_offset, min_periods=1).mean()\n",
    "        ma_col_name = f\"{col}_MA_{w_label}\"\n",
    "        df_velocity_ma[ma_col_name] = ma\n",
    "\n",
    "# Concat original velocity data with its moving averages\n",
    "df_velocity_with_ma = pd.concat([df_Inflöde_FT10101_mps, df_velocity_ma], axis=1)\n",
    "\n",
    "print(\"Velocity DataFrame with Moving Averages:\")\n",
    "print(f\"Shape: {df_velocity_with_ma.shape}\")\n",
    "print(f\"Columns: {df_velocity_with_ma.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_velocity_with_ma.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "492786de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed df_Utflöde_Bräddning_LT23101:\n",
      "df_Utflöde_Bräddning_LT23101.head() =                      Utflöde Bräddning LT23101\n",
      "DateTime                                      \n",
      "2024-11-01 03:01:00                        0.0\n",
      "2024-11-01 03:02:00                        0.0\n",
      "2024-11-01 03:03:00                        0.0\n",
      "2024-11-01 03:04:00                        0.0\n",
      "2024-11-01 03:05:00                        0.0\n",
      "df_Utflöde_Bräddning_LT23101: 513892 rows\n"
     ]
    }
   ],
   "source": [
    "# Compute overflow rate from level df_Nivå_Bräddning_LT23101 -> df_Utflöde_Bräddning_LT23101\n",
    "import numpy as np\n",
    "\n",
    "# Constants for the V-notch weir formula (Excel equivalent):\n",
    "# =IF(E2<0.25;0;0.58*8/15*TAN(RADIANS(100)/2) * (E2-H)^(2.5) * SQRT(2*g) * 3600)\n",
    "H_threshold = 0.25  # m, crest level H in the formula\n",
    "angle_deg = 100.0   # degrees for the V-notch angle\n",
    "g = 9.81            # m/s^2\n",
    "\n",
    "# Prepare head above crest (clipped at 0)\n",
    "level_col = 'Nivå Bräddning LT23101'\n",
    "head = (df_Nivå_Bräddning_LT23101[level_col] - H_threshold).clip(lower=0.0)\n",
    "\n",
    "# Precompute constant multiplier K = 0.58*8/15*TAN(RADIANS(100)/2)*SQRT(2*g)*3600\n",
    "K = 0.58 * (8.0/15.0) * np.tan(np.radians(angle_deg)/2.0) * np.sqrt(2.0 * g) * 3600.0\n",
    "\n",
    "# Flow [m3/h]\n",
    "flow_m3h = K * np.power(head, 2.5)\n",
    "\n",
    "# Build resulting DataFrame\n",
    "df_Utflöde_Bräddning_LT23101 = pd.DataFrame(\n",
    "    data={'Utflöde Bräddning LT23101': flow_m3h},\n",
    "    index=df_Nivå_Bräddning_LT23101.index\n",
    ")\n",
    "\n",
    "# Check for and remove duplicates in the overflow DataFrame\n",
    "if df_Utflöde_Bräddning_LT23101.index.duplicated().any():\n",
    "    dup_count = df_Utflöde_Bräddning_LT23101.index.duplicated().sum()\n",
    "    print(f\"Warning: df_Utflöde_Bräddning_LT23101 has {dup_count} duplicate indices. Keeping first occurrence.\")\n",
    "    df_Utflöde_Bräddning_LT23101 = df_Utflöde_Bräddning_LT23101[~df_Utflöde_Bräddning_LT23101.index.duplicated(keep='first')]\n",
    "\n",
    "# Optional diagnostics\n",
    "print('Computed df_Utflöde_Bräddning_LT23101:')\n",
    "dprint(df_Utflöde_Bräddning_LT23101.head())\n",
    "print(f'df_Utflöde_Bräddning_LT23101: {len(df_Utflöde_Bräddning_LT23101)} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71a7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a9a47df",
   "metadata": {},
   "source": [
    "# Calculate Moving Averages and Flow Plant Balance\n",
    "Now that duplicates have been removed, we can safely calculate moving averages with different window sizes to smooth the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29672c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned lengths (L, R): 513892 513892\n",
      "Left NaNs total: 53\n",
      "Right NaNs total: 0\n",
      "len(df_flows.columns) = 16\n",
      "len(df_flowdiff.columns) = 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate moving averages for each column (time-based windows)\n",
    "# Each row in df_ax represents 1 minute, so use time-based rolling windows\n",
    "import pandas as pd\n",
    "\n",
    "# First, merge overflow flow into df_ax so MAs can be calculated for it\n",
    "df_ax = pd.merge(df_ax, df_Utflöde_Bräddning_LT23101, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Prepare container for moving averages\n",
    "df_ma = pd.DataFrame(index=df_ax.index)\n",
    "\n",
    "# Define time-based windows (labels -> pandas offset strings)\n",
    "windows = { '1h': '60min', '24h': '24h', '7d': '7D' }\n",
    "\n",
    "for col in df_ax.columns:\n",
    "    for w_label, w_offset in windows.items():\n",
    "        # Use time-based rolling which is robust to missing/irregular timestamps\n",
    "        ma = df_ax[col].rolling(w_offset, min_periods=1).mean()\n",
    "        ma_col_name = f\"{col}_MA_{w_label}\"\n",
    "        df_ma[ma_col_name] = ma\n",
    "\n",
    "# Compute differences (Inflöde - Utflöde - Bräddning) for each moving-average window in a separate DataFrame\n",
    "# Source column names in df_ax are 'Inflöde FT-10101', 'Utflöde FT-72101', and 'Utflöde Bräddning LT23101'\n",
    "df_ma_diff = pd.DataFrame(index=df_ax.index)\n",
    "inflow_main = 'Inflöde FT-10101'\n",
    "outflow_main = 'Utflöde FT-72101'\n",
    "inflow_externslam = 'Inflöde Extenslam FT80101'\n",
    "outflow_bräddning = 'Utflöde Bräddning LT23101'\n",
    "\n",
    "for w_label in windows.keys():\n",
    "    col_inflow_main = f\"{inflow_main}_MA_{w_label}\"\n",
    "    col_outflow_main = f\"{outflow_main}_MA_{w_label}\"\n",
    "    col_outflow_externslam = f\"{inflow_externslam}_MA_{w_label}\"\n",
    "    col_outflow_bräddning = f\"{outflow_bräddning}_MA_{w_label}\"\n",
    "    diff_col = f\"Diff_MA_{w_label}\"\n",
    "    if col_inflow_main in df_ma.columns and col_outflow_main in df_ma.columns and col_outflow_bräddning in df_ma.columns:\n",
    "        df_ma_diff[diff_col] = (df_ma[col_inflow_main] + df_ma[col_outflow_externslam] - df_ma[col_outflow_main] - df_ma[col_outflow_bräddning])/(df_ma[col_inflow_main] + df_ma[col_outflow_externslam])\n",
    "    else:\n",
    "        # If one of the MA columns is missing, create the diff column with NaNs and warn\n",
    "        df_ma_diff[diff_col] = pd.NA\n",
    "        print(f\"Warning: cannot compute {diff_col} because one or more required columns are missing\")\n",
    "\n",
    "# Concat all data side-by-side for left axis (includes original values and their moving averages)\n",
    "df_flows = pd.concat([df_ax, df_ma], axis=1)\n",
    "\n",
    "# Concat all data side-by-side for right axis (only flow differences)\n",
    "df_flowdiff = df_ma_diff\n",
    "\n",
    "# Align left and right frames to a common union index (sorted)\n",
    "union_idx = df_flows.index.union(df_flowdiff.index)\n",
    "try:\n",
    "    union_idx = union_idx.unique()\n",
    "except Exception:\n",
    "    pass\n",
    "union_idx = union_idx.sort_values()\n",
    "\n",
    "df_flows = df_flows.reindex(union_idx)\n",
    "df_flowdiff = df_flowdiff.reindex(union_idx)\n",
    "\n",
    "# Optional: sanity prints\n",
    "print(\"Aligned lengths (L, R):\", len(df_flows.index), len(df_flowdiff.index))\n",
    "print(\"Left NaNs total:\", int(df_flows.isna().sum().sum()))\n",
    "print(\"Right NaNs total:\", int(df_flowdiff.isna().sum().sum()))\n",
    "\n",
    "# Debug: number of columns\n",
    "dprint(len(df_flows.columns))\n",
    "dprint(len(df_flowdiff.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "605ed4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate timestamps DataFrame shape: (12, 6)\n",
      "\n",
      "First few duplicate rows:\n",
      "                     Inflöde FT-10101  Utflöde FT-72101  \\\n",
      "DateTime                                                  \n",
      "2025-07-07 07:47:00               NaN               NaN   \n",
      "2025-07-07 07:47:00               NaN               NaN   \n",
      "2025-07-07 07:47:00               NaN               NaN   \n",
      "2025-07-07 07:47:00               NaN               NaN   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "2025-07-07 07:47:00         94.965556         38.107736   \n",
      "\n",
      "                     Inflöde Extenslam FT80101                        Source  \\\n",
      "DateTime                                                                       \n",
      "2025-07-07 07:47:00                        0.0  df_Inflöde_Extenslam_FT80101   \n",
      "2025-07-07 07:47:00                        0.0  df_Inflöde_Extenslam_FT80101   \n",
      "2025-07-07 07:47:00                        NaN     df_Nivå_Bräddning_LT23101   \n",
      "2025-07-07 07:47:00                        NaN     df_Nivå_Bräddning_LT23101   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "2025-07-07 07:47:00                        0.0                         df_ax   \n",
      "\n",
      "                          DuplicateGroup  Nivå Bräddning LT23101  \n",
      "DateTime                                                          \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                0.151694  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                0.151694  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "2025-07-07 07:47:00  2025-07-07 07:47:00                     NaN  \n",
      "\n",
      "Duplicates by source:\n",
      "Source\n",
      "df_Inflöde_Extenslam_FT80101    2\n",
      "df_Nivå_Bräddning_LT23101       2\n",
      "df_ax                           8\n",
      "dtype: int64\n",
      "\n",
      "Unique duplicate timestamps:\n",
      "['2025-07-07 07:47:00']\n"
     ]
    }
   ],
   "source": [
    "# Display duplicate timestamps if any were found\n",
    "if 'df_duplicate_timestamps' in locals() and not df_duplicate_timestamps.empty:\n",
    "    print(f\"Duplicate timestamps DataFrame shape: {df_duplicate_timestamps.shape}\")\n",
    "    print(\"\\nFirst few duplicate rows:\")\n",
    "    print(df_duplicate_timestamps.head(20))\n",
    "    \n",
    "    # Show summary by source\n",
    "    print(\"\\nDuplicates by source:\")\n",
    "    print(df_duplicate_timestamps.groupby('Source').size())\n",
    "    \n",
    "    # Show the actual duplicate timestamps\n",
    "    print(\"\\nUnique duplicate timestamps:\")\n",
    "    print(sorted(df_duplicate_timestamps.index.unique().strftime('%Y-%m-%d %H:%M:%S').tolist()))\n",
    "else:\n",
    "    print(\"No duplicates were found or df_duplicate_timestamps is empty.\")\n",
    "    print(\"\\nNote: If you previously had duplicates, you may need to:\")\n",
    "    print(\"1. Re-run the data loading cells (cells 4-5)\")\n",
    "    print(\"2. Then re-run the moving averages cell to capture duplicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b9be0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUPLICATE TIMESTAMPS SUMMARY\n",
      "============================================================\n",
      "Total duplicate rows captured: 12\n",
      "Number of unique duplicate timestamps: 1\n",
      "\n",
      "Duplicates by source DataFrame:\n",
      "  df_Inflöde_Extenslam_FT80101: 2 rows\n",
      "  df_Nivå_Bräddning_LT23101: 2 rows\n",
      "  df_ax: 8 rows\n",
      "\n",
      "Unique timestamps that appear multiple times:\n",
      "  2025-07-07 07:47:00\n",
      "\n",
      "Example: Data for first duplicate timestamp\n",
      "                     Inflöde FT-10101  Utflöde FT-72101  Inflöde Extenslam FT80101                        Source       DuplicateGroup  Nivå Bräddning LT23101\n",
      "DateTime                                                                                                                                                     \n",
      "2025-07-07 07:47:00               NaN               NaN                        0.0  df_Inflöde_Extenslam_FT80101  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00               NaN               NaN                        0.0  df_Inflöde_Extenslam_FT80101  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00               NaN               NaN                        NaN     df_Nivå_Bräddning_LT23101  2025-07-07 07:47:00                0.151694\n",
      "2025-07-07 07:47:00               NaN               NaN                        NaN     df_Nivå_Bräddning_LT23101  2025-07-07 07:47:00                0.151694\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "2025-07-07 07:47:00         94.965556         38.107736                        0.0                         df_ax  2025-07-07 07:47:00                     NaN\n",
      "\n",
      "Duplicate timestamps saved to: duplicate_timestamps.csv\n"
     ]
    }
   ],
   "source": [
    "# Summary of duplicate timestamps\n",
    "if not df_duplicate_timestamps.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DUPLICATE TIMESTAMPS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total duplicate rows captured: {len(df_duplicate_timestamps)}\")\n",
    "    print(f\"Number of unique duplicate timestamps: {df_duplicate_timestamps.index.nunique()}\")\n",
    "    \n",
    "    print(\"\\nDuplicates by source DataFrame:\")\n",
    "    source_counts = df_duplicate_timestamps.groupby('Source').size()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} rows\")\n",
    "    \n",
    "    print(\"\\nUnique timestamps that appear multiple times:\")\n",
    "    unique_dups = sorted(df_duplicate_timestamps.index.unique())\n",
    "    for ts in unique_dups:\n",
    "        print(f\"  {ts.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Show a sample of the data for one duplicate timestamp\n",
    "    print(\"\\nExample: Data for first duplicate timestamp\")\n",
    "    first_dup = unique_dups[0]\n",
    "    sample = df_duplicate_timestamps[df_duplicate_timestamps.index == first_dup]\n",
    "    print(sample.to_string())\n",
    "    \n",
    "    # Optionally save to CSV\n",
    "    csv_path = 'duplicate_timestamps.csv'\n",
    "    df_duplicate_timestamps.to_csv(csv_path)\n",
    "    print(f\"\\nDuplicate timestamps saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45de502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chrini\\sourceGit\\chrisgladious\\InteractiveFlowCharts\\src\\InteractivePlotWindow.py:981: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  self.canvas.draw()\n"
     ]
    }
   ],
   "source": [
    "from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys as _sys\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def _make_and_show():\n",
    "        app = QApplication.instance() or QApplication(_sys.argv)\n",
    "        mainWin = IPW(df_axL = df_flows,\n",
    "                            df_axL_Title = 'Flöde & Bräddflöde [m3/h]', \n",
    "                            df_axR = df_flowdiff, \n",
    "                            df_axR_Title = 'Flödesdiff [%]',\n",
    "                            WindowTitle='Pajala ARV Flöde',\n",
    "                            settings_file='InteractivePlotWindow.json::Flöden och Flödesdiff'\n",
    "                        )\n",
    "        mainWin.show()\n",
    "        # Keep references to avoid garbage collection in notebook kernels.\n",
    "        \n",
    "        # Store on the app and module globals so the objects persist after this function returns.\n",
    "        try:\n",
    "            app._pajala_mainWin = mainWin\n",
    "        except Exception:\n",
    "            pass\n",
    "        globals()['_pajala_mainWin'] = mainWin\n",
    "        globals()['_pajala_app'] = app\n",
    "        return app\n",
    "\n",
    "    # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "    if 'ipykernel' in _sys.modules:\n",
    "        try:\n",
    "            ip = get_ipython()\n",
    "            if ip is not None:\n",
    "                # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                ip.run_line_magic('gui', 'qt')\n",
    "        except Exception:\n",
    "            ip = None\n",
    "        # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "        app = _make_and_show()\n",
    "        # Keep references in the IPython user namespace if available so users can interact with them\n",
    "        if ip is not None:\n",
    "            try:\n",
    "                ip.user_ns['_pajala_app'] = app\n",
    "                ip.user_ns['_pajala_mainWin'] = globals().get('_pajala_mainWin')\n",
    "            except Exception:\n",
    "                # Fall back to module globals (already set by _make_and_show)\n",
    "                pass\n",
    "    else:\n",
    "        # Running as a script: start the blocking event loop\n",
    "        app = _make_and_show()\n",
    "        _sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294b2a1",
   "metadata": {},
   "source": [
    "# Velocity Chart (FT-10101)\n",
    "Interactive plot showing flow velocity in m/s with flow differences on the right axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "845f1d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned velocity DataFrame shape: (513892, 4)\n",
      "Aligned flowdiff DataFrame shape: (513892, 3)\n"
     ]
    }
   ],
   "source": [
    "# from InteractivePlotWindow import InteractivePlotWindow as IPW\n",
    "# from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "# Align the velocity (with MAs) and flowdiff DataFrames to the same index\n",
    "union_idx_velocity = df_velocity_with_ma.index.union(df_flowdiff.index)\n",
    "df_velocity_aligned = df_velocity_with_ma.reindex(union_idx_velocity)\n",
    "df_flowdiff_aligned = df_flowdiff.reindex(union_idx_velocity)\n",
    "\n",
    "print(f\"Aligned velocity DataFrame shape: {df_velocity_aligned.shape}\")\n",
    "print(f\"Aligned flowdiff DataFrame shape: {df_flowdiff_aligned.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys as _sys\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def _make_and_show_velocity():\n",
    "        app = QApplication.instance() or QApplication(_sys.argv)\n",
    "        mainWin_velocity = IPW(\n",
    "            df_axL = df_velocity_aligned,\n",
    "            df_axL_Title = 'Hastighet FT-10101 [m/s]', \n",
    "            df_axR = df_flowdiff_aligned, \n",
    "            df_axR_Title = 'Flödesdiff [%]',\n",
    "            WindowTitle='Pajala ARV - Hastighet FT-10101',\n",
    "            settings_file='InteractivePlotWindow.json::Hastighet FT-10101 och Flödesdiff'\n",
    "        )\n",
    "        mainWin_velocity.show()\n",
    "        # Keep references to avoid garbage collection in notebook kernels.\n",
    "        \n",
    "        # Store on the app and module globals so the objects persist after this function returns.\n",
    "        try:\n",
    "            app._pajala_velocity_mainWin = mainWin_velocity\n",
    "        except Exception:\n",
    "            pass\n",
    "        globals()['_pajala_velocity_mainWin'] = mainWin_velocity\n",
    "        globals()['_pajala_velocity_app'] = app\n",
    "        return app\n",
    "\n",
    "    # If running inside an IPython kernel (notebook), request IPython to enable the Qt event loop\n",
    "    if 'ipykernel' in _sys.modules:\n",
    "        try:\n",
    "            ip = get_ipython()\n",
    "            if ip is not None:\n",
    "                # enable GUI event loop integration; this avoids a blocking app.exec() call\n",
    "                ip.run_line_magic('gui', 'qt')\n",
    "        except Exception:\n",
    "            ip = None\n",
    "        # Create and show window but do NOT call app.exec() - the event loop is managed by IPython\n",
    "        app = _make_and_show_velocity()\n",
    "        # Keep references in the IPython user namespace if available so users can interact with them\n",
    "        if ip is not None:\n",
    "            try:\n",
    "                ip.user_ns['_pajala_velocity_app'] = app\n",
    "                ip.user_ns['_pajala_velocity_mainWin'] = globals().get('_pajala_velocity_mainWin')\n",
    "            except Exception:\n",
    "                # Fall back to module globals (already set by _make_and_show_velocity)\n",
    "                pass\n",
    "    else:\n",
    "        # Running as a script: start the blocking event loop\n",
    "        app = _make_and_show_velocity()\n",
    "        _sys.exit(app.exec())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
